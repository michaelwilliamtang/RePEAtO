{"cells":[{"cell_type":"markdown","source":["## IMPORTS AND ALL THAT"],"metadata":{"id":"ZvlUS3gGFu9f"},"id":"ZvlUS3gGFu9f"},{"cell_type":"code","source":["%%capture\n","!pip install datasets\n","!pip install numpy\n","!pip install pandas\n","!pip3 install http://download.pytorch.org/whl/cu92/torch-0.4.1-cp36-cp36m-linux_x86_64.whl\n","!pip3 install torchvision\n","\n","!pip install simcse\n","\n","!pip install gensim==4.1.2\n","!pip install cython\n","!pip install nltk"],"metadata":{"id":"aucLqNev5jNX","executionInfo":{"status":"ok","timestamp":1651547405367,"user_tz":240,"elapsed":40483,"user":{"displayName":"Evan Dogariu","userId":"08207052186530176995"}}},"id":"aucLqNev5jNX","execution_count":1,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n","import random\n","random.seed(10)\n","torch.manual_seed(0)\n","np.random.seed(0)\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","import re\n","import time\n","import math\n","from IPython.utils import io\n","\n","from tqdm import tqdm\n","\n","import nltk\n","nltk.download('punkt')\n","from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n","from simcse import SimCSE\n","from nltk.tokenize import word_tokenize\n","\n","from gensim.models import FastText\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o-FM39kEF4yu","executionInfo":{"status":"ok","timestamp":1651547426573,"user_tz":240,"elapsed":21211,"user":{"displayName":"Evan Dogariu","userId":"08207052186530176995"}},"outputId":"14565701-5791-401a-965b-6c2a0458f684"},"id":"o-FM39kEF4yu","execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"markdown","source":["Make sure to add 484-finalProject as shortcut in drive."],"metadata":{"id":"xiGy1ifZGDOw"},"id":"xiGy1ifZGDOw"},{"cell_type":"code","source":["cd /content/drive/My\\ Drive/484-finalProject"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KA9TfA1RGB6n","executionInfo":{"status":"ok","timestamp":1651547426574,"user_tz":240,"elapsed":12,"user":{"displayName":"Evan Dogariu","userId":"08207052186530176995"}},"outputId":"c403b3ef-11c6-442e-c991-906059fe444e"},"id":"KA9TfA1RGB6n","execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/.shortcut-targets-by-id/154EdVBqpeIxqbeun-wGvxSgsYCVV1wdV/484-finalProject\n"]}]},{"cell_type":"markdown","source":["##PREPARE DATASET"],"metadata":{"id":"2PJZlXY0CR7F"},"id":"2PJZlXY0CR7F"},{"cell_type":"code","execution_count":4,"id":"bfb33fc8","metadata":{"id":"bfb33fc8","executionInfo":{"status":"ok","timestamp":1651547427123,"user_tz":240,"elapsed":560,"user":{"displayName":"Evan Dogariu","userId":"08207052186530176995"}}},"outputs":[],"source":["import datasets\n","from datasets import load_dataset, list_datasets\n","import pandas as pd \n","import re \n","import numpy as np "]},{"cell_type":"code","execution_count":5,"id":"3bb659e0","metadata":{"id":"3bb659e0","executionInfo":{"status":"ok","timestamp":1651547427124,"user_tz":240,"elapsed":3,"user":{"displayName":"Evan Dogariu","userId":"08207052186530176995"}}},"outputs":[],"source":["MAX_DEPTH = 8\n","\n","class Node(object):\n","    '''\n","    each node contains \n","    - parent \n","    - children \n","    - text\n","    '''\n","    def __init__(self,txt: str, level:int):\n","        self.text = txt \n","        self.level = level\n","        self.parent = None \n","        self.children = []\n","    def insertChild(self,child):\n","        self.children.append(child)\n","    def linkParent(self, parent):\n","        if(self.parent != None):\n","            print(\"ERROR: node \", self.text, \"already has a parent\")\n","        else:\n","            self.parent = parent \n","            \n","        \n","class Tree(object):\n","    def __init__(self,document):\n","        self.root = Node(document['title'],level=0)\n","        self.depth = np.amax([v['type'] for v in document['document']], initial=0)\n","        curNode = self.root \n","        # para of format {\"text\", \"type\"}\n","        for para in document['document']:\n","            newNode = Node(para['text'],para['type'])\n","            \n","            # growing in depth\n","            if(newNode.level == -1 or newNode.level > curNode.level):\n","                curNode.insertChild(newNode)\n","                newNode.linkParent(curNode)\n","                if(newNode.level > 0):\n","                    curNode = newNode \n","                \n","            # new heading belong to the same or lower level of subheading \n","            else: \n","                # trace back to the heading level that new heading is immediately under \n","                while(curNode.level>=newNode.level):\n","                    curNode = curNode.parent\n","                curNode.insertChild(newNode)\n","                newNode.linkParent(curNode)\n","                curNode = newNode \n","        return \n","    \n","    def printTree(self):\n","        print(\"======== PRINTING TREE =========\")\n","        print(\"TITLE: \", self.root.text)\n","        print(\"MAX DEPTH: \", self.depth)\n","        print(\"===============================\")\n","        def printNode(curNode):\n","            print(curNode.text)\n","            if(curNode.level == -1):\n","                return \n","            \n","            for child in curNode.children:\n","                printNode(child)\n","            return \n","        printNode(self.root)       "]},{"cell_type":"code","execution_count":6,"id":"f09a8de0","metadata":{"id":"f09a8de0","executionInfo":{"status":"ok","timestamp":1651547427263,"user_tz":240,"elapsed":142,"user":{"displayName":"Evan Dogariu","userId":"08207052186530176995"}}},"outputs":[],"source":["## helper functions \n","\n","# get type of text \n","def checkHeading(txt):\n","    if(txt == ''):\n","        return -2\n","    if(re.search(r'^\\s=.+\\s=\\s\\n',txt)):\n","        return int(len(re.findall(r'\\s=',txt))/2 - 1)\n","    return -1 \n","\n","# load documents to feed to tree \n","def createDocuments(data):\n","    documents_with = []\n","    documents_without = []\n","    document_with = []\n","    document_without = []\n","    curTitle = ''\n","    for i in data:\n","        c = checkHeading(i)\n","        if(c==-2):\n","            continue\n","        if(c>-1):\n","            # strip heading \n","            i = re.findall(r'=\\s([^=]+)\\s=', i)[0]\n","        if(c==0):\n","            \n","            # clear out empty headings \n","            while(len(document_with)>1 and document_with[-1]['type']!=-1):\n","                document_with.pop(-1)\n","            documents_with.append({'title': curTitle, 'document':document_with})\n","            documents_without.append(document_without)\n","            curTitle = i\n","            document_with = []\n","            document_without = []\n","            \n","        else:\n","            # clear out empty headings GOOFY HELP HOW TO CLEAN THIS UP \n","            if(len(document_with)>1 and document_with[-1]['type']!=-1 and c <= document_with[-1]['type'] and c!=-1):\n","                document_with.pop(-1)\n","            document_with.append({'text':i,'type':c})\n","            if(c==-1):\n","                document_without.append(i)\n","            \n","    documents_with.pop(0)\n","    documents_without.pop(0)\n","    return documents_with, documents_without"]},{"cell_type":"markdown","id":"e7ac6222","metadata":{"id":"e7ac6222"},"source":["loadData() creates a list of data points containing the title of article, raw text (paragraphs), and the tree representation of heading structures."]},{"cell_type":"code","execution_count":7,"id":"e6f4ecfe","metadata":{"id":"e6f4ecfe","executionInfo":{"status":"ok","timestamp":1651547427263,"user_tz":240,"elapsed":10,"user":{"displayName":"Evan Dogariu","userId":"08207052186530176995"}}},"outputs":[],"source":["## load wiki dataset \n","def loadData(train = False, min_size=-1):\n","    \"\"\"\n","    prepare dataset for training, which is a list of dictionaries containing:  \n","    - document title (string)\n","    - paragraphs (list of string)\n","    - tree representation of headings\n","    \"\"\"\n","    \n","    data_raw = load_dataset(\"wikitext\",'wikitext-103-v1',split='train' if train else 'test')\n","    data_raw = data_raw['text']\n","    documents_with, documents_without = createDocuments(data_raw)\n","    \n","    data = []\n","    i = 0\n","    for document in documents_with:\n","        tree = Tree(document)\n","        if len(documents_without[i]) < min_size:\n","          continue\n","#         tree.printTree()\n","        data.append({\n","            \"title\":document['title'],\n","            \"paragraphs\":documents_without[i],\n","            \"tree\": tree\n","        })\n","        i+=1\n","    return data    "]},{"cell_type":"markdown","source":["## DA LCA"],"metadata":{"id":"7dU8iPy-CgQO"},"id":"7dU8iPy-CgQO"},{"cell_type":"markdown","source":["##### Preparing LCA loss evaluation function + tools for trees (please run!)"],"metadata":{"id":"PDWF2apsjZT1"},"id":"PDWF2apsjZT1"},{"cell_type":"code","source":["# tree-related helper functions\n","\n","# iterate over a tree rooted at node in preorder traversal\n","def preorder(node):\n","    if len(node.children) == 0:\n","        yield node\n","    for ch in node.children:\n","        yield from preorder(ch)\n","\n","# only prints leaves, i.e. text representations of paragraph\n","# note: depends on accurate text, level population\n","# text should be indices\n","def print_tree(curNode):\n","    if curNode.level == -1:\n","        print(curNode.text, end='')\n","        return\n","    print('[', end='')\n","    for idx, child in enumerate(curNode.children):\n","        print_tree(child)\n","        if idx < len(curNode.children) - 1:\n","            print(', ', end='')\n","    print(']', end='')\n","    if curNode.level == 0:\n","        print() # final print after entire tree is printed\n","\n","# text should be snippets\n","def print_snippet_tree(curNode, indent='  '):\n","    if curNode.level == -1:\n","        print(indent + curNode.text, end='')\n","        return\n","    print(indent+'[heading]')\n","    for idx, child in enumerate(curNode.children):\n","        print_snippet_tree(child, indent+'  ')\n","        if idx < len(curNode.children) - 1:\n","            print()\n","    if curNode.level == 0:\n","        print() # final print after entire tree is printed\n","\n","def clone_tree(root):\n","    root_copy = Node(root.text, root.level)\n","    for ch in root.children:\n","        root_copy.insertChild(clone_tree(ch))\n","        root_copy.children[-1].linkParent(root_copy)\n","    return root_copy\n","\n","# return indexified tree with text as paragraphs to text as indices of paragraphs for more concise printing\n","def indexified_tree(root):\n","    root_copy = clone_tree(root)\n","    for idx, node in enumerate(preorder(root_copy)):\n","        node.text = idx\n","    return root_copy\n","\n","# return indexified tree with text as paragraphs to text as indices of paragraphs for more concise printing\n","def textified_tree(root, paras):\n","    root_copy = clone_tree(root)\n","    for idx, node in enumerate(preorder(root_copy)):\n","        node.text = paras[idx][:40] + '...'\n","    return root_copy\n","\n","# print_tree(roots[0])\n","# print_tree(train_y[0].root)"],"metadata":{"id":"IBXXXBS4jjcT","executionInfo":{"status":"ok","timestamp":1651558904181,"user_tz":240,"elapsed":123,"user":{"displayName":"Evan Dogariu","userId":"08207052186530176995"}}},"id":"IBXXXBS4jjcT","execution_count":227,"outputs":[]},{"cell_type":"code","source":["# lca-related helper functions and lca loss\n","# note: assumed indexified trees\n","def trace_helper(node, i, trace):\n","    if node.text == i:\n","        return True\n","    for idx, ch in enumerate(node.children):\n","        if trace_helper(ch, i, trace):\n","            trace.append(idx)\n","            return True\n","    return False\n","\n","def get_trace(root, i):\n","    trace = []\n","    trace_helper(root, i, trace)\n","    trace.reverse()\n","    return trace\n","\n","def compute_lca_dist(root, i, j):\n","    trace_i = get_trace(root, i)\n","    trace_j = get_trace(root, j)\n","    # print(trace_i)\n","    # print(trace_j)\n","    for idx in range(min(len(trace_i), len(trace_j))):\n","        if trace_i[idx] != trace_j[idx]:\n","            return len(trace_i) + len(trace_j) - 2 * idx\n","    return len(trace_i) + len(trace_j)\n","\n","# def lca_loss(root1, root2, num_paras):\n","#     loss = 0\n","#     for i in range(1, num_paras+1): # 1-indexed from indexify\n","#         for j in range(i+1, num_paras+1):\n","#             dist1 = compute_lca_dist(root1, i, j)\n","#             dist2 = compute_lca_dist(root2, i, j)\n","#             # print(i, j, dist1, dist2)\n","#             loss += (dist1 - dist2) * (dist1 - dist2)\n","#     if num_paras == 1:\n","#         return loss\n","#     return loss / num_paras / (num_paras - 1) * 2\n","\n","def lca_loss(root1, root2, num_paras):\n","    loss = 0\n","    for i in range(2, num_paras+1): # 1-indexed from indexify\n","        j = i-1\n","        dist1 = compute_lca_dist(root1, i, j)\n","        dist2 = compute_lca_dist(root2, i, j)\n","        # print(i, j, dist1, dist2)\n","        loss += (dist1 - dist2) * (dist1 - dist2)\n","    if num_paras == 1:\n","        return loss\n","    return loss / (num_paras - 1)\n","\n","def batch_lca_loss(roots1, roots2, num_paras):\n","    tt = 0\n","    for root1, root2, num in zip(roots1, roots2, num_paras):\n","        tt += lca_loss(root1, root2, num)\n","    return tt / len(roots1)"],"metadata":{"id":"z9hZqeHXjef7","executionInfo":{"status":"ok","timestamp":1651547427393,"user_tz":240,"elapsed":4,"user":{"displayName":"Evan Dogariu","userId":"08207052186530176995"}}},"id":"z9hZqeHXjef7","execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["## DA MODEL / TRAINING"],"metadata":{"id":"9PbhxTS2XoyW"},"id":"9PbhxTS2XoyW"},{"cell_type":"markdown","source":["##### Model 3: Recursive split MLP -- note this code may have a decoding bug"],"metadata":{"id":"yc5HGcLzgoFv"},"id":"yc5HGcLzgoFv"},{"cell_type":"code","source":["def recur_search(node, to_be_marked, curr_data):\n","  if node.level == -1:\n","    curr_data.append(([n.level for n in to_be_marked], node.parent.level + 1))  # add data of which nodes you are first of, and also what is your depth\n","    to_be_marked = []\n","    return to_be_marked\n","  to_be_marked.append(node)\n","  for child in node.children:\n","    to_be_marked = recur_search(child, to_be_marked, curr_data)\n","  return to_be_marked\n","\n","def convert_dataset(data, window_size):\n","  paras, trees = [d['paragraphs'] for d in data], [d['tree'] for d in data]\n","  dataX, dataD, datay = [], [], []\n","  for i in range(len(paras)):  # for each article\n","    article = paras[i]\n","    curr_data = []\n","    recur_search(trees[i].root, [], curr_data)\n","    for p in range(len(article)):  # for each paragraph\n","      context = []\n","      for j in range(p - window_size, p + window_size + 1):  # for para in context\n","        if j < 0 or j >= len(article):\n","          context.append(None)\n","        else:\n","          context.append(article[j])\n","      breaks, depth = curr_data[p]\n","      print(breaks, '  d=', depth, '    ', article[p][:20])\n","      for d in range(0 if p == 0 else 1, depth):\n","        dataX.append(context)\n","        dataD.append([d])\n","        datay.append([1 if d in breaks else 0])\n","  dataD = torch.tensor(np.array(dataD))\n","  datay = torch.tensor(np.array(datay))\n","  return dataX, dataD, datay"],"metadata":{"id":"cvpuvYJnhci0","executionInfo":{"status":"ok","timestamp":1651547427525,"user_tz":240,"elapsed":135,"user":{"displayName":"Evan Dogariu","userId":"08207052186530176995"}}},"id":"cvpuvYJnhci0","execution_count":10,"outputs":[]},{"cell_type":"code","source":["def get_split_data(data, window_size, train_ratio=0.8, val_ratio=0.0001, test_ratio=0.0001):\n","    random.shuffle(data)\n","    n_data = len(data)\n","    train_idx = int(train_ratio * n_data)\n","    val_idx = int((train_ratio + val_ratio) * n_data)\n","    test_idx = int((train_ratio + val_ratio + test_ratio) * n_data)\n","    print('getting training data')\n","    train_data = convert_dataset(data[:train_idx], window_size=window_size)\n","    print('getting val data')\n","    val_data = convert_dataset(data[train_idx:val_idx], window_size=window_size)\n","    print('getting test data')\n","    test_data = convert_dataset(data[val_idx:test_idx], window_size=window_size)\n","    return train_data, val_data, test_data"],"metadata":{"id":"1Ce1cQI_hjKm","executionInfo":{"status":"ok","timestamp":1651547427526,"user_tz":240,"elapsed":4,"user":{"displayName":"Evan Dogariu","userId":"08207052186530176995"}}},"id":"1Ce1cQI_hjKm","execution_count":11,"outputs":[]},{"cell_type":"code","source":["# iterate over batches of data and labels\n","def batch_iter(data, batch_size, shuffle=False):\n","    X, D, y = data\n","    batch_num = math.ceil(len(X) / batch_size)\n","    index_array = list(range(len(X)))\n","\n","    if shuffle:\n","        np.random.shuffle(index_array)\n","\n","    for i in range(batch_num):\n","        indices = index_array[i * batch_size: (i + 1) * batch_size]\n","        batch_data_X = [X[idx] for idx in indices]\n","        batch_data_D = D[indices]\n","        batch_data_y = y[indices]\n","\n","        yield batch_data_X, batch_data_D, batch_data_y"],"metadata":{"id":"D5oIYe9shkio","executionInfo":{"status":"ok","timestamp":1651547427526,"user_tz":240,"elapsed":4,"user":{"displayName":"Evan Dogariu","userId":"08207052186530176995"}}},"id":"D5oIYe9shkio","execution_count":12,"outputs":[]},{"cell_type":"code","source":["# MODULES FOR EMBEDDING 3 DIFFERENT WAYS: DOC2VEC, PROJECTED SIMCSE, PROJECTED FASTTEXT\n","# each one takes a batch of lists of paragraphs, outputs a batch of concatenated paragraph embeddings\n","# forward pass input: batch (len B) of list of paragraphs (len 2 * window_size + 1), each para variable length\n","# forward pass output: tensor of size B x ((2 * window_size + 1) * emb_dim)\n","class Doc2VecEmbedding(nn.Module):\n","  def __init__(self, window_size, emb_dim): \n","    super().__init__()\n","    self.emb_dim = emb_dim\n","    self.doc2vec = Doc2Vec.load(\"models/{}_d2v.model\".format(emb_dim))\n","\n","  def forward(self, x):\n","    with torch.no_grad():\n","      batch = []\n","      for b in x:\n","        paras = []\n","        for p in b:\n","          paras.append(self.doc2vec.infer_vector(word_tokenize(p.lower())) if p is not None else np.zeros(shape=(self.emb_dim)))\n","        batch.append(np.concatenate(paras, axis=0))\n","      return torch.tensor(np.stack(batch, axis=0)).to(device)\n","\n","class SimCSEEmbedding(nn.Module):\n","  def __init__(self, window_size, emb_dim, dropout): \n","    super().__init__()\n","    self.window_size = window_size\n","    self.emb_dim = emb_dim\n","    self.simcse = SimCSE(\"princeton-nlp/sup-simcse-bert-base-uncased\")\n","    self.SIMCSE_DIM = 768 # dim of simcse sentence embeddings\n","    self.lstm = nn.LSTM(input_size=self.SIMCSE_DIM,\n","                        hidden_size=int(emb_dim / 4),\n","                        num_layers=1,\n","                        bidirectional=True,\n","                        batch_first=True, \n","                        dropout=0.0).to(device)\n","\n","  def forward(self, x):\n","    B = len(x)  # batch size\n","    batch = []\n","    with torch.no_grad():\n","      for b in x:\n","        for p in b:\n","          if p is not None:\n","            p = re.sub('\\n', '', p)\n","            sents = re.split('[.]|[!]|[?]', p.strip())\n","            with io.capture_output() as captured:\n","                sents_emb = self.simcse.encode(sents, device=device, batch_size=len(sents), max_length=128)  # a tensor of len(sents) x SIMCSE_DIM\n","            batch.append(sents_emb)\n","          else:\n","            batch.append(torch.zeros((1, self.SIMCSE_DIM), device=device))\n","      batch_emb = torch.nn.utils.rnn.pad_sequence(batch, batch_first=True).to(device)\n","      packed_in = torch.nn.utils.rnn.pack_padded_sequence(batch_emb, torch.tensor([a.shape[0] for a in batch_emb]), batch_first=True, enforce_sorted=False)\n","\n","    _, (hidden, cell) = self.lstm(packed_in.to(device))\n","    para_embs = torch.cat((hidden[0], cell[0], hidden[1], cell[1]), dim=1)        \n","      \n","    return para_embs.reshape(B, (2 * self.window_size + 1) * self.emb_dim)\n","\n","\n","class FastTextEmbedding(nn.Module):\n","  def __init__(self, window_size, emb_dim, dropout): \n","    super().__init__()\n","    self.window_size = window_size\n","    self.emb_dim = emb_dim\n","    self.fasttext = FastText.load_fasttext_format('models/fast-text-300.bin').wv\n","    self.FASTTEXT_DIM = 300 # dim of fasttext word embeddings\n","    self.lstm = nn.LSTM(input_size=self.FASTTEXT_DIM,\n","                        hidden_size=int(emb_dim / 4),\n","                        num_layers=1,\n","                        bidirectional=True,\n","                        batch_first=True, \n","                        dropout=0.0).to(device)\n","\n","  def forward(self, x):\n","    B = len(x)  # batch size\n","    batch = []\n","    with torch.no_grad():\n","      for b in x:\n","        for p in b:\n","          if p is not None:\n","            p = re.sub('\\n', '', p)\n","            words = re.sub(\"[^\\s\\w]\", \"\", p.strip()).split(' ')\n","            words = list(filter(None, words))\n","            if len(words) == 0:\n","                batch.append(torch.zeros((1, self.FASTTEXT_DIM)))\n","                continue\n","            words_emb = torch.stack([torch.tensor(self.fasttext[word]) for word in words]).to(device)  # a tensor of len(words) x FASTTEXT_DIM\n","            batch.append(words_emb)\n","          else:\n","            batch.append(torch.zeros((1, self.FASTTEXT_DIM), device=device))\n","      batch_emb = torch.nn.utils.rnn.pad_sequence(batch, batch_first=True).to(device)\n","      packed_in = torch.nn.utils.rnn.pack_padded_sequence(batch_emb, torch.tensor([a.shape[0] for a in batch_emb]), batch_first=True, enforce_sorted=False)\n","\n","    _, (hidden, cell) = self.lstm(packed_in.to(device))\n","    para_embs = torch.cat((hidden[0], cell[0], hidden[1], cell[1]), dim=1)        \n","      \n","    return para_embs.reshape(B, (2 * self.window_size + 1) * self.emb_dim)"],"metadata":{"id":"BAtSWSpo5hMJ","executionInfo":{"status":"ok","timestamp":1651547427630,"user_tz":240,"elapsed":107,"user":{"displayName":"Evan Dogariu","userId":"08207052186530176995"}}},"id":"BAtSWSpo5hMJ","execution_count":13,"outputs":[]},{"cell_type":"code","source":["class MLP(nn.Module):\n","  def __init__(self, layer_dims, window_size, emb_dim, emb_method, dropout=0.1):\n","        super().__init__()\n","        self.window_size = window_size\n","        self.emb_dim = emb_dim # dimension of each paragraph embedding\n","        if emb_method == 'doc2vec':\n","          self.emb = Doc2VecEmbedding(window_size, emb_dim)\n","        elif emb_method == 'simcse':\n","          self.emb = SimCSEEmbedding(window_size, emb_dim, dropout=dropout)\n","        elif emb_method == 'fasttext':\n","          self.emb = FastTextEmbedding(window_size, emb_dim, dropout=dropout)\n","        else:\n","          raise NotImplementedError()\n","\n","        in_feats = (2 * window_size + 1) * self.emb_dim\n","        self.layers = []\n","        for dim in layer_dims:\n","          self.layers.append(nn.Linear(in_features=in_feats + 1, out_features=dim))\n","          in_feats = dim\n","        self.layers.append(nn.Linear(in_features=in_feats + 1, out_features=1))\n","        self.layers = nn.ModuleList(self.layers)\n","        self.dropout = nn.Dropout(dropout)\n","\n","  def forward(self, x, d):\n","      '''\n","      x is a batch (list) of windows (list) of paragraphs, which are strings\n","      d is the depths for the whole batch (tensor)\n","      '''\n","      B = len(x)\n","      x = self.emb(x).float()  # x is a (B x (2W + 1) x E) tensor\n","      x = x.reshape(B, (2 * self.window_size + 1) * self.emb_dim)\n","      for layer in self.layers[:-1]:\n","        x = F.relu(layer(torch.cat((x, d), dim=1)))\n","      x = self.dropout(x)\n","      x = torch.sigmoid(self.layers[-1](torch.cat((x, d), dim=1)))\n","      return x\n","\n","  def recursive_outline(self, indices, node, threshold, contexts, wordy):\n","    window_size = int((len(contexts[0]) - 1) / 2)\n","    if len(indices) == 0:\n","        return\n","    if len(indices) == 1:\n","        new = Node(contexts[indices[0]][window_size], -1)\n","        new.linkParent(node)\n","        node.insertChild(new)\n","        return\n","    outs = []\n","    for i in indices:\n","        X = [contexts[i]]\n","        D = torch.tensor([node.level + 1]).to(device).unsqueeze(dim=0)\n","        out = self.forward(X, D).squeeze()\n","        if wordy:\n","          print(out.cpu().item(), D.cpu().item(), '       ', contexts[i][window_size][:40])\n","        t = threshold if node.level < 1 else 4 * threshold * (1.2 ** node.level)\n","        outs.append(out.cpu().item() > t)\n","    prev = 0\n","    flag = True\n","    for o in range(1, len(outs)):\n","        if outs[o]:\n","            if o - prev > 1:\n","              new = Node('', node.level + 1)\n","              new.linkParent(node)\n","              node.insertChild(new)\n","              self.recursive_outline(indices[prev:o], new, threshold, contexts, wordy)\n","            elif o - prev == 1:\n","              new = Node(contexts[indices[prev]][window_size], -1)\n","              new.linkParent(node)\n","              node.insertChild(new)\n","              prev = o\n","              continue\n","            else:\n","              continue\n","            prev = o\n","            flag = False\n","    if flag:\n","      for i in indices:\n","        new = Node(contexts[i][window_size], -1)\n","        new.linkParent(node)\n","        node.insertChild(new)\n","    else:\n","       new = Node('', node.level + 1)\n","       new.linkParent(node)\n","       node.insertChild(new)\n","       self.recursive_outline(indices[prev:], new, threshold, contexts, wordy)\n","    return\n","\n","  def outline(self, article, threshold=0.77, wordy=False):\n","      self.eval()\n","      contexts = []\n","      for p in range(len(article)):\n","        context = []\n","        for j in range(p - self.window_size, p + self.window_size + 1):\n","            if j < 0 or j >= len(article):\n","                context.append(None)\n","            else:\n","                context.append(article[j])\n","        contexts.append(context)\n","\n","      indices = list(range(len(article)))  # indices to recur with\n","      root = Node('root', 0)\n","      self.recursive_outline(indices, root, threshold, contexts, wordy)\n","\n","      def printNode(curNode):\n","          print(curNode.level, '       ', curNode.text[:50])\n","          if curNode.level == -1:\n","              return\n","\n","          for child in curNode.children:\n","              printNode(child)\n","          return\n","\n","      if wordy:\n","        printNode(root)\n","      return root\n","\n","  def save(self, path: str):\n","      \"\"\" Save the model to a file.\n","      @param path (str): path to the model\n","      \"\"\"\n","\n","      params = {\n","          # 'args': dict(hid_dim=self.hid_dim, n_layers=self.n_layers, num_heads=self.num_heads,\n","          #       num_enc_layers=self.num_enc_layers, num_dec_layers=self.num_dec_layers, ff_dim=self.ff_dim, dropout=self.dropout),\n","          'state_dict': self.state_dict()\n","      }\n","\n","      torch.save(params, path)\n","\n","# # RUN THIS CELL FOR DOC2VEC MLP MODEL, with either SMALL or LARGE to indicate model size\n","# LARGE = False  # make this true to use larger model\n","# MODEL_PATH = 'checkpoints/{}'.format('large_mlp_doc2vec.bin' if LARGE else 'mlp_doc2vec.bin')\n","# # MODEL_PATH = 'checkpoints/{}'.format('mlp_big_windowdoc2vec.bin')\n","\n","\n","# WINDOW_SIZE = 4 if LARGE else 3  # number of neighbors to consider in each direction\n","# EMB_DIM = 256  # dim each paragraph becomes, via magic :)\n","# EMB_METHOD = 'doc2vec'  # one of 'doc2vec', 'simcse', 'fasttext'\n","# MLP_ARCHITECTURE = [5096, 1024, 256, 64] if LARGE else [1024, 256, 64]  # sizes of hidden layers in MLP\n","\n","#-----------------------------------------------------------\n","\n","# MODEL_PATH = 'checkpoints/{}'.format('test_mlp_doc2vec.bin')\n","# WINDOW_SIZE = 3\n","# EMB_DIM = 256\n","# EMB_METHOD = 'doc2vec'\n","# MLP_ARCHITECTURE = [1024, 256, 64]\n","\n","# model = MLP(layer_dims=MLP_ARCHITECTURE, window_size=WINDOW_SIZE, emb_dim=EMB_DIM, emb_method=EMB_METHOD)\n","\n","# # EVAL STUFF\n","# params = torch.load(MODEL_PATH, map_location=lambda storage, loc: storage)\n","# model.load_state_dict(params['state_dict'])\n","# model = model.to(device)\n","# model.eval()\n","\n","# test = loadData(train=False)\n","\n","# sample = test[0:1]\n","# print('inferencing!')\n","# roots = []\n","# # for i in tqdm(range(len(sample))):\n","# #   roots.append(indexified_tree(model.outline(sample[i]['paragraphs'], threshold=1, wordy=False)))\n","# for i in tqdm(range(len(sample))):\n","#   model.outline(sample[i]['paragraphs'], threshold=0.7, wordy=False)\n","# # golds = [indexified_tree(a['tree'].root) for a in sample]\n","# # lens = [len(a['paragraphs']) for a in sample]\n","# # print(batch_lca_loss(roots, golds, lens))\n","\n","# print()\n","# print(sample[0]['tree'].printTree())"],"metadata":{"id":"K2ayqAuIhqjt","executionInfo":{"status":"ok","timestamp":1651557094803,"user_tz":240,"elapsed":383,"user":{"displayName":"Evan Dogariu","userId":"08207052186530176995"}}},"id":"K2ayqAuIhqjt","execution_count":211,"outputs":[]},{"cell_type":"code","source":["def train(model, train_data, val_data, lr=0.002, batch_size=32, grad_clip=5.0, lr_decay=0.5,\n","          max_epoch=50, log_every=5, valid_niter=25, max_patience=4, max_num_trial=5, model_path='mlp.bin'):\n","    model.train()\n","    model.float()\n","\n","    # # initialize model parameters\n","    # for p in model.parameters():\n","    #     p.data.uniform_(-0.1, 0.1)\n","\n","    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","    print('use device: %s' % device)\n","\n","    model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n","    print('{} parameters!'.format(sum([np.prod(p.size()) for p in model_parameters])))\n","\n","    model = model.to(device)\n","    optimizer = torch.optim.Adam(model.parameters(), lr)\n","\n","    num_trial = 0\n","    train_iter = patience = cum_loss = report_loss = cum_tgt_words = report_tgt_words = 0\n","    cum_examples = report_examples = epoch = valid_num = 0\n","    hist_valid_scores = []\n","    train_time = begin_time = time.time()\n","    print('begin Maximum Likelihood training')\n","\n","    train_losses = []\n","    val_losses = []\n","    loss_fn = nn.BCELoss(reduction='sum')\n","    \n","    while True:\n","        epoch += 1\n","        batch_num = math.ceil(len(train_data[0]) / batch_size)\n","        current_iter = 0\n","        for batch in batch_iter(train_data, batch_size=batch_size, shuffle=True):\n","            X, D, y = batch\n","            D = D.to(device)\n","            y = y.to(dtype=torch.float32, device=device)\n","\n","            model.train()\n","            current_iter += 1\n","            train_iter += 1\n","\n","            optimizer.zero_grad()\n","            batch_size = len(X)\n","            out = model(X, D)\n","            train_loss = loss_fn(out, y)\n","            train_loss.backward()\n","\n","            # clip gradient\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n","\n","            optimizer.step()\n","\n","            report_loss += train_loss.item()\n","            cum_loss += train_loss.item()\n","            report_examples += batch_size\n","            cum_examples += batch_size\n","\n","            if train_iter % log_every == 0:\n","                print('epoch %d (%d / %d), iter %d, avg train loss %f, '\n","                      'cum examples %d, time elapsed %.2f sec' %\n","                      (epoch, current_iter, batch_num, train_iter,\n","                       report_loss / report_examples,\n","                       cum_examples,\n","                       time.time() - begin_time))\n","\n","                train_time = time.time()\n","                report_loss = report_examples = 0.\n","\n","            # perform validation\n","            if train_iter % valid_niter == 0:\n","                model.eval()\n","                with torch.no_grad():\n","                    print('epoch %d, iter %d, cum loss %f, cum examples %d' % (epoch, train_iter,\n","                            cum_loss / cum_examples,\n","                            cum_examples))\n","                    train_losses.append(cum_loss / cum_examples)\n","                    cum_loss = cum_examples = 0.\n","\n","                    print('begin validation ...')\n","\n","                    val_cum_loss = 0.\n","                    val_cum_examples = 0\n","\n","                    count = 0\n","                    NUM_BATCHES = 16  # number of batches to validate over each time\n","                    for batch in batch_iter(val_data, batch_size, shuffle=True):\n","                        if count >= NUM_BATCHES:\n","                            break\n","                        X, D, y = batch\n","                        D = D.to(device)\n","                        y = y.to(dtype=torch.float32, device=device)\n","\n","                        batch_size = len(X)\n","                        out = model(X, D)\n","                        val_loss = loss_fn(out, y)\n","                        val_cum_loss += val_loss.item()\n","                        val_cum_examples += batch_size\n","                        count += 1\n","\n","                    val_losses.append(val_cum_loss / val_cum_examples)\n","                    valid_metric = -val_cum_loss / val_cum_examples # metric for evaluating whether model is improving on val data\n","\n","                    print('validation: iter %d, val loss %f' % (train_iter, val_cum_loss / val_cum_examples))\n","\n","                    is_better = len(hist_valid_scores) == 0 or valid_metric > max(hist_valid_scores)\n","                    hist_valid_scores.append(valid_metric)\n","\n","                    if is_better:\n","                        patience = 0\n","                        print('epoch %d, iter %d: save currently the best model to [%s]' %\n","                                (epoch, train_iter, model_path))\n","                        model.save(model_path)\n","                        torch.save(optimizer.state_dict(), model_path + '.optim')\n","                        np.save('{}_train.npy'.format(model_path.split('.')[-2]), np.array(train_losses))\n","                        np.save('{}_val.npy'.format(model_path.split('.')[-2]), np.array(val_losses))\n","                    elif patience < max_patience:\n","                        patience += 1\n","                        print('hit patience %d' % patience)\n","\n","                        if patience == max_patience:\n","                            num_trial += 1\n","                            print('hit #%d trial' % num_trial)\n","                            if num_trial == max_num_trial:\n","                                print('early stop!')\n","                                exit(0)\n","\n","                            # decay lr, and restore from previously best checkpoint\n","                            lr = optimizer.param_groups[0]['lr'] * lr_decay\n","                            print('load previously best model and decay learning rate to %f' % lr)\n","\n","                            # load model\n","                            params = torch.load(model_path, map_location=lambda storage, loc: storage)\n","                            model.load_state_dict(params['state_dict'])\n","                            model = model.to(device)\n","                            train_losses = list(np.load('{}_train.npy'.format(model_path.split('.')[-2]), allow_pickle=True))\n","                            val_losses = list(np.load('{}_val.npy'.format(model_path.split('.')[-2]), allow_pickle=True))\n","\n","                            print('restore parameters of the optimizers')\n","                            optimizer.load_state_dict(torch.load(model_path + '.optim'))\n","\n","                            # set new lr\n","                            for param_group in optimizer.param_groups:\n","                                param_group['lr'] = lr\n","\n","                            # reset patience\n","                            patience = 0\n","\n","        if epoch == max_epoch:\n","            print('reached maximum number of epochs!')\n","            break"],"metadata":{"id":"kFhfncT-hruW","executionInfo":{"status":"ok","timestamp":1651547428690,"user_tz":240,"elapsed":453,"user":{"displayName":"Evan Dogariu","userId":"08207052186530176995"}}},"id":"kFhfncT-hruW","execution_count":15,"outputs":[]},{"cell_type":"markdown","source":["## DA USER INTERACTION AREA"],"metadata":{"id":"48Iah0w1Xtzz"},"id":"48Iah0w1Xtzz"},{"cell_type":"markdown","source":["### Doc2Vec MLP"],"metadata":{"id":"cf_1yMg3inCH"},"id":"cf_1yMg3inCH"},{"cell_type":"code","source":["# RUN THIS CELL FOR DOC2VEC MLP MODEL, with either SMALL or LARGE to indicate model size\n","LARGE = True  # make this true to use larger model\n","MODEL_PATH = 'checkpoints/{}'.format('test_large_mlp_doc2vec.bin' if LARGE else 'test_mlp_doc2vec.bin')\n","# MODEL_PATH = 'checkpoints/{}'.format('mlp_big_windowdoc2vec.bin')\n","\n","\n","WINDOW_SIZE = 4 if LARGE else 3  # number of neighbors to consider in each direction\n","EMB_DIM = 256  # dim each paragraph becomes, via magic :)\n","EMB_METHOD = 'doc2vec'  # one of 'doc2vec', 'simcse', 'fasttext'\n","MLP_ARCHITECTURE = [5096, 1024, 256, 64] if LARGE else [1024, 256, 64]  # sizes of hidden layers in MLP\n","\n","model = MLP(layer_dims=MLP_ARCHITECTURE, window_size=WINDOW_SIZE, emb_dim=EMB_DIM, emb_method=EMB_METHOD)"],"metadata":{"id":"Ufjb3hKAhsGr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651558456344,"user_tz":240,"elapsed":12934,"user":{"displayName":"Evan Dogariu","userId":"08207052186530176995"}},"outputId":"5b289f50-38d5-4ae3-8ae1-207467141cd9"},"id":"Ufjb3hKAhsGr","execution_count":216,"outputs":[{"output_type":"stream","name":"stderr","text":["05/03/2022 06:14:03 - INFO - gensim.utils -   loading Doc2Vec object from models/256_d2v.model\n","05/03/2022 06:14:04 - INFO - gensim.utils -   loading dv recursively from models/256_d2v.model.dv.* with mmap=None\n","05/03/2022 06:14:04 - INFO - gensim.utils -   loading vectors from models/256_d2v.model.dv.vectors.npy with mmap=None\n","05/03/2022 06:14:08 - INFO - gensim.utils -   loading wv recursively from models/256_d2v.model.wv.* with mmap=None\n","05/03/2022 06:14:08 - INFO - gensim.utils -   loading vectors from models/256_d2v.model.wv.vectors.npy with mmap=None\n","05/03/2022 06:14:10 - INFO - gensim.utils -   loading syn1neg from models/256_d2v.model.syn1neg.npy with mmap=None\n","05/03/2022 06:14:13 - INFO - gensim.utils -   setting ignored attribute cum_table to None\n","05/03/2022 06:14:15 - INFO - gensim.utils -   Doc2Vec lifecycle event {'fname': 'models/256_d2v.model', 'datetime': '2022-05-03T06:14:15.922184', 'gensim': '4.1.2', 'python': '3.7.13 (default, Apr 24 2022, 01:04:09) \\n[GCC 7.5.0]', 'platform': 'Linux-5.4.188+-x86_64-with-Ubuntu-18.04-bionic', 'event': 'loaded'}\n"]}]},{"cell_type":"markdown","source":["### FastText MLP"],"metadata":{"id":"W89nqnKMir1F"},"id":"W89nqnKMir1F"},{"cell_type":"code","source":["# RUN THIS CELL FOR FASTTEXT MLP MODEL, with either SMALL or LARGE to indicate model size\n","LARGE = True  # make this true to use larger model\n","MODEL_PATH = 'checkpoints/{}'.format('test_large_mlp_fasttext.bin' if LARGE else 'mlp_fasttext.bin')\n","\n","\n","WINDOW_SIZE = 4 if LARGE else 2  # number of neighbors to consider in each direction\n","EMB_DIM = 512 if LARGE else 256  # dim each paragraph becomes, via magic :)\n","EMB_METHOD = 'fasttext'  # one of 'doc2vec', 'simcse', 'fasttext'\n","MLP_ARCHITECTURE = [5096, 1024, 256, 64] if LARGE else [1024, 256, 64]  # sizes of hidden layers in MLP\n","\n","model = MLP(layer_dims=MLP_ARCHITECTURE, window_size=WINDOW_SIZE, emb_dim=EMB_DIM, emb_method=EMB_METHOD)"],"metadata":{"id":"n_i2-gzLh14q","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1651558440449,"user_tz":240,"elapsed":3129,"user":{"displayName":"Evan Dogariu","userId":"08207052186530176995"}},"outputId":"175bab30-f369-4ef0-f1db-82d74bc25752"},"id":"n_i2-gzLh14q","execution_count":215,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:63: DeprecationWarning: Call to deprecated `load_fasttext_format` (use load_facebook_vectors (to use pretrained embeddings) or load_facebook_model (to continue training with the loaded full model, more RAM) instead).\n","05/03/2022 06:13:57 - INFO - gensim.models._fasttext_bin -   loading 2000000 words for fastText model from models/fast-text-300.bin\n","05/03/2022 06:14:00 - ERROR - root -   Internal Python error in the inspect module.\n","Below is the traceback from this internal error.\n","\n","05/03/2022 06:14:00 - INFO - root -   \n","Unfortunately, your original traceback can not be constructed.\n","\n"]},{"output_type":"stream","name":"stdout","text":["Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"<ipython-input-215-b48641caf9a7>\", line 11, in <module>\n","    model = MLP(layer_dims=MLP_ARCHITECTURE, window_size=WINDOW_SIZE, emb_dim=EMB_DIM, emb_method=EMB_METHOD)\n","  File \"<ipython-input-211-7e4e1e11ef37>\", line 11, in __init__\n","    self.emb = FastTextEmbedding(window_size, emb_dim, dropout=dropout)\n","  File \"<ipython-input-13-87f18e25ddf7>\", line 63, in __init__\n","    self.fasttext = FastText.load_fasttext_format('models/fast-text-300.bin')\n","  File \"/usr/local/lib/python3.7/dist-packages/gensim/utils.py\", line 1521, in new_func1\n","    return func(*args, **kwargs)\n","  File \"/usr/local/lib/python3.7/dist-packages/gensim/models/fasttext.py\", line 580, in load_fasttext_format\n","    return load_facebook_model(model_file, encoding=encoding)\n","  File \"/usr/local/lib/python3.7/dist-packages/gensim/models/fasttext.py\", line 728, in load_facebook_model\n","    return _load_fasttext_format(path, encoding=encoding, full_model=True)\n","  File \"/usr/local/lib/python3.7/dist-packages/gensim/models/fasttext.py\", line 808, in _load_fasttext_format\n","    m = gensim.models._fasttext_bin.load(fin, encoding=encoding, full_model=full_model)\n","  File \"/usr/local/lib/python3.7/dist-packages/gensim/models/_fasttext_bin.py\", line 345, in load\n","    raw_vocab, vocab_size, nwords, ntokens = _load_vocab(fin, new_format, encoding=encoding)\n","  File \"/usr/local/lib/python3.7/dist-packages/gensim/models/_fasttext_bin.py\", line 213, in _load_vocab\n","    char_byte = fin.read(1)\n","KeyboardInterrupt\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 1823, in showtraceback\n","    stb = value._render_traceback_()\n","AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/ultratb.py\", line 1132, in get_records\n","    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n","  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/ultratb.py\", line 313, in wrapped\n","    return f(*args, **kwargs)\n","  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/ultratb.py\", line 358, in _fixed_getinnerframes\n","    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n","  File \"/usr/lib/python3.7/inspect.py\", line 1502, in getinnerframes\n","    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n","  File \"/usr/lib/python3.7/inspect.py\", line 1460, in getframeinfo\n","    filename = getsourcefile(frame) or getfile(frame)\n","  File \"/usr/lib/python3.7/inspect.py\", line 696, in getsourcefile\n","    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n","  File \"/usr/lib/python3.7/inspect.py\", line 725, in getmodule\n","    file = getabsfile(object, _filename)\n","  File \"/usr/lib/python3.7/inspect.py\", line 709, in getabsfile\n","    return os.path.normcase(os.path.abspath(_filename))\n","  File \"/usr/lib/python3.7/posixpath.py\", line 383, in abspath\n","    cwd = os.getcwd()\n","FileNotFoundError: [Errno 2] No such file or directory\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"]}]},{"cell_type":"markdown","source":["### SimCSE MLP"],"metadata":{"id":"hOZ0OO7oiu5-"},"id":"hOZ0OO7oiu5-"},{"cell_type":"code","source":["# RUN THIS CELL FOR SIMCSE MLP MODEL\n","# MODEL_PATH = 'checkpoints/{}'.format('evan_mlp_simcse.bin')\n","MODEL_PATH = 'checkpoints/{}'.format('mlp_simcse.bin')\n","\n","\n","WINDOW_SIZE = 3  # number of neighbors to consider in each direction\n","EMB_DIM = 256  # dim each paragraph becomes, via magic :)\n","EMB_METHOD = 'simcse'  # one of 'doc2vec', 'simcse', 'fasttext'\n","MLP_ARCHITECTURE = [1024, 256, 64]  # sizes of hidden layers in MLP\n","\n","model = MLP(layer_dims=MLP_ARCHITECTURE, window_size=WINDOW_SIZE, emb_dim=EMB_DIM, emb_method=EMB_METHOD)"],"metadata":{"id":"HuIsKPy2h3RF"},"id":"HuIsKPy2h3RF","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Train or Eval"],"metadata":{"id":"hUg2QmdYjsO2"},"id":"hUg2QmdYjsO2"},{"cell_type":"code","source":["# TRAINING STUFF\n","data = loadData(train=True)\n","train_data, val_data, test_data = get_split_data(data, window_size=WINDOW_SIZE, train_ratio=0.65, val_ratio=0.3, test_ratio=0.01)\n","train(model, train_data, val_data, lr=0.002, batch_size=64, grad_clip=5.0, lr_decay=0.5,\n","      max_epoch=50, log_every=5, valid_niter=50, max_patience=4, max_num_trial=5, model_path=MODEL_PATH)"],"metadata":{"id":"B9KIlXR6RL7y","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651553638101,"user_tz":240,"elapsed":23836,"user":{"displayName":"Evan Dogariu","userId":"08207052186530176995"}},"outputId":"03762ef1-433f-482e-8941-6ab7e6851c8e"},"id":"B9KIlXR6RL7y","execution_count":91,"outputs":[{"output_type":"stream","name":"stderr","text":["05/03/2022 04:53:36 - WARNING - datasets.builder -   Reusing dataset wikitext (/root/.cache/huggingface/datasets/wikitext/wikitext-103-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\n"]}]},{"cell_type":"code","source":["# EVAL STUFF\n","params = torch.load(MODEL_PATH, map_location=lambda storage, loc: storage)\n","model.load_state_dict(params['state_dict'])\n","model = model.to(device)\n","model.eval()\n","\n","# test = loadData(train=False)\n","\n","sample = data[23672:23673]\n","# sample = test\n","# print('inferencing!')\n","roots = []\n","for i in range(len(sample)):\n","  out = model.outline(sample[i]['paragraphs'], threshold=0.17, wordy=False)\n","  roots.append(indexified_tree(out))\n","print()\n","\n","# roots = [indexified_tree(model.outline(a['paragraphs'])) for a in sample]\n","golds = [indexified_tree(a['tree'].root) for a in sample]\n","lens = [len(a['paragraphs']) for a in sample]\n","# print(batch_lca_loss(roots, golds, lens))\n","\n","print_snippet_tree(textified_tree(out, sample[0]['paragraphs']))\n","\n","# print()\n","# print(sample[0]['tree'].printTree())"],"metadata":{"id":"3hy4A11xhwzd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651558962471,"user_tz":240,"elapsed":1598,"user":{"displayName":"Evan Dogariu","userId":"08207052186530176995"}},"outputId":"bc126bef-2b4f-4827-aa26-0cbff4f62509"},"id":"3hy4A11xhwzd","execution_count":237,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","  [heading]\n","     Dan Dugan ( born March 20 , 1943 ) is a...\n","     In his youth , Dugan was fascinated by ...\n","     Dugan first recorded sounds in the late...\n","    [heading]\n","       Daniel W. Dugan was born in Los Angeles...\n","       Dugan changed from lighting design to s...\n","       Dugan designed sound for three regional...\n","       When Margrit Mondavi founded the Mondav...\n","       Dugan occasionally delivered speeches a...\n","       While designing sound for the musical H...\n","       Though the algorithm was good , the ref...\n","       \" I was messing around with logarithmic...\n","       Dugan licensed this more practical syst...\n","       In the late 1980s , Dugan developed a g...\n","       Dugan 's original 1974 patent expired i...\n","       In September 2006 , Dugan produced the ...\n","       In February 2011 , Dugan demonstrated a...\n","       Dugan made his first sound effects reco...\n","       Dugan and his wife Sharon Perry , the N...\n","    [heading]\n","       \" There are three potential values in s...\n","       In 2006 , Dugan assisted a group of res...\n","    [heading]\n","       In 1998 an organization he co @-@ found...\n","       As co @-@ founder and Secretary of PLAN...\n"]}]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"},"colab":{"name":"MLP bible.ipynb","provenance":[{"file_id":"1c7s9vjiKUkJweHr4sWgQjobFNgNY8gxR","timestamp":1651462918288},{"file_id":"1MGfIx5mwYvwfJTGO2w5ySa7D-1S4-FOd","timestamp":1650353357566}],"collapsed_sections":["ZvlUS3gGFu9f","2PJZlXY0CR7F","W89nqnKMir1F","hOZ0OO7oiu5-"]},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}