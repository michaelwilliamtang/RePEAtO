{"cells":[{"cell_type":"markdown","metadata":{"id":"0ba86871"},"source":["##### Load Wikitext dataset (please run!)"],"id":"0ba86871"},{"cell_type":"markdown","metadata":{"id":"8LbF0QkbhDYL"},"source":["Headings without any text below it (i.g. only table) are excluded."],"id":"8LbF0QkbhDYL"},{"cell_type":"code","source":["%%capture\n","!pip install transformers"],"metadata":{"id":"M4w7U1As7SXc","executionInfo":{"status":"ok","timestamp":1651541875336,"user_tz":240,"elapsed":9905,"user":{"displayName":"Michael Tang","userId":"18009171711496870090"}}},"id":"M4w7U1As7SXc","execution_count":2,"outputs":[]},{"cell_type":"code","source":["# !pip install simcse"],"metadata":{"id":"qVnrEIoPu_Lj","executionInfo":{"status":"ok","timestamp":1651541875337,"user_tz":240,"elapsed":12,"user":{"displayName":"Michael Tang","userId":"18009171711496870090"}}},"id":"qVnrEIoPu_Lj","execution_count":3,"outputs":[]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":21572,"status":"ok","timestamp":1651541896899,"user":{"displayName":"Michael Tang","userId":"18009171711496870090"},"user_tz":240},"id":"aucLqNev5jNX"},"outputs":[],"source":["%%capture \n","!pip install datasets\n","!pip install numpy\n","!pip install pandas\n","!pip3 install http://download.pytorch.org/whl/cu92/torch-0.4.1-cp36-cp36m-linux_x86_64.whl\n","!pip3 install torchvision"],"id":"aucLqNev5jNX"},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":5921,"status":"ok","timestamp":1651541902816,"user":{"displayName":"Michael Tang","userId":"18009171711496870090"},"user_tz":240},"id":"bfb33fc8"},"outputs":[],"source":["import datasets\n","from datasets import load_dataset, list_datasets\n","import pandas as pd \n","import re \n","import numpy as np "],"id":"bfb33fc8"},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1651541902817,"user":{"displayName":"Michael Tang","userId":"18009171711496870090"},"user_tz":240},"id":"3bb659e0"},"outputs":[],"source":["class Node(object):\n","    '''\n","    each node contains \n","    - parent \n","    - children \n","    - text\n","    '''\n","    def __init__(self,txt: str, level:int):\n","        self.text = txt \n","        self.level = level\n","        self.parent = None \n","        self.children = []\n","    def insertChild(self,child):\n","        self.children.append(child)\n","    def linkParent(self, parent):\n","        if(self.parent != None):\n","            print(\"ERROR: node \", self.text, \"already has a parent\")\n","        else:\n","            self.parent = parent \n","            \n","        \n","class Tree(object):\n","    def __init__(self,document):\n","        self.root = Node(document['title'],level=0)\n","        self.depth = np.amax([v['type'] for v in document['document']], initial=0)\n","        curNode = self.root \n","        # para of format {\"text\", \"type\"}\n","        for para in document['document']:\n","            newNode = Node(para['text'],para['type'])\n","            \n","            # growing in depth\n","            if(newNode.level == -1 or newNode.level > curNode.level):\n","                curNode.insertChild(newNode)\n","                newNode.linkParent(curNode)\n","                if(newNode.level > 0):\n","                    curNode = newNode \n","                \n","            # new heading belong to the same or lower level of subheading \n","            else: \n","                # trace back to the heading level that new heading is immediately under \n","                while(curNode.level>=newNode.level):\n","                    curNode = curNode.parent\n","                curNode.insertChild(newNode)\n","                newNode.linkParent(curNode)\n","                curNode = newNode \n","        return \n","    \n","    def printTree(self):\n","        print(\"======== PRINTING TREE =========\")\n","        print(\"TITLE: \", self.root.text)\n","        print(\"MAX DEPTH: \", self.depth)\n","        print(\"===============================\")\n","        def printNode(curNode):\n","            print(curNode.text)\n","            if(curNode.level == -1):\n","                return \n","            \n","            for child in curNode.children:\n","                printNode(child)\n","            return \n","        printNode(self.root)\n","        \n","    \n","\n","        "],"id":"3bb659e0"},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1651541902818,"user":{"displayName":"Michael Tang","userId":"18009171711496870090"},"user_tz":240},"id":"f09a8de0"},"outputs":[],"source":["## helper functions \n","\n","# get type of text \n","def checkHeading(txt):\n","    if(txt == ''):\n","        return -2\n","    if(re.search(r'^\\s=.+\\s=\\s\\n',txt)):\n","        return int(len(re.findall(r'\\s=',txt))/2 - 1)\n","    return -1 \n","\n","# load documents to feed to tree \n","def createDocuments(data):\n","    documents_with = []\n","    documents_without = []\n","    document_with = []\n","    document_without = []\n","    curTitle = ''\n","    for i in data:\n","        c = checkHeading(i)\n","        if(c==-2):\n","            continue\n","        if(c>-1):\n","            # strip heading \n","            i = re.findall(r'=\\s([^=]+)\\s=', i)[0]\n","        if(c==0):\n","            \n","            # clear out empty headings \n","            while(len(document_with)>1 and document_with[-1]['type']!=-1):\n","                document_with.pop(-1)\n","            documents_with.append({'title': curTitle, 'document':document_with})\n","            documents_without.append(document_without)\n","            curTitle = i\n","            document_with = []\n","            document_without = []\n","            \n","        else:\n","            # clear out empty headings GOOFY HELP HOW TO CLEAN THIS UP \n","            if(len(document_with)>1 and document_with[-1]['type']!=-1 and c <= document_with[-1]['type'] and c!=-1):\n","                document_with.pop(-1)\n","            document_with.append({'text':i,'type':c})\n","            if(c==-1):\n","                document_without.append(i)\n","            \n","    documents_with.pop(0)\n","    documents_without.pop(0)\n","    return documents_with, documents_without"],"id":"f09a8de0"},{"cell_type":"markdown","metadata":{"id":"e7ac6222"},"source":["loadData() creates a list of data points containing the title of article, raw text (paragraphs), and the tree representation of heading structures."],"id":"e7ac6222"},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1651541902819,"user":{"displayName":"Michael Tang","userId":"18009171711496870090"},"user_tz":240},"id":"e6f4ecfe"},"outputs":[],"source":["## load wiki dataset \n","def loadData(train = False, min_size=-1):\n","    \"\"\"\n","    prepare dataset for training, which is a list of dictionaries containing:  \n","    - document title (string)\n","    - paragraphs (list of string)\n","    - tree representation of headings\n","    \"\"\"\n","    \n","    data_raw = load_dataset(\"wikitext\",'wikitext-103-v1',split='train' if train else 'test')\n","    data_raw = data_raw['text']\n","    documents_with, documents_without = createDocuments(data_raw)\n","    \n","    data = []\n","    i = 0\n","    for document in documents_with:\n","        tree = Tree(document)\n","        if len(documents_without[i]) < min_size:\n","          continue\n","#         tree.printTree()\n","        data.append({\n","            \"title\":document['title'],\n","            \"paragraphs\":documents_without[i],\n","            \"tree\": tree\n","        })\n","        i+=1\n","    return data    "],"id":"e6f4ecfe"},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":168,"referenced_widgets":["c1f56010394a4de6ad0d6606512961bf","fc7773686c0345859691d1012b46300e","7bedbae29b864d54b2d2aa71305a7e85","7c52fa240a584372b3d2fe641e0a26c4","c2b75d0567d1405c825e9420e296761e","2f4732f535394dc583c773bccd4064fc","e9c2052cd89e4fab9654e820a4191085","df7b6449ca5d4af692726a9fa0627b87","a6b53968bcc041c5bf1b41ce39fdea91","bc410147e6274f2285d5c170ac3705a1","bf53104080a547e08a3d42d7a096c621","b4c2854f450944c3ac4e1e2e2c8c52fc","52e6283ba63141108d1733cc934c566a","fff4a0959cd8400394b5769b3a51a497","63275b065caa4e578eff42ce9c24bfd2","bb77ca82ab46482b91b2dac242805d8d","f553d8ae53d74da182d51a65735934d6","70f6c6080bcd4772b038bf6b4d66ce71","3f83868c2b7f45d1b30c359c1b460357","585a521fab47472eb13254ff05e67be3","00cc2499804b4b81b860087d81c90e2d","2944ead59ab549468af252bbaa220070","f7ab22541968478f85b6dc94f766ad95","1f217b335fca4de189857609b4e3ea54","fa2e652439334b8cb7af8c1248bbc764","a1bd627fcf204cd48f93d23bfe22ab5d","dbbcd5d2fdcf4d408c47e3f016937489","0e0f715c376c47f08603643d1d834480","d3f6d546db024d88a003e61fbf7d3609","ef9a194b5b624c6b9cdf91829ebd7acb","d24770109d8b4b49870195597844a69c","94cf94ba79d54a8f8e6f4a6f974a2e13","e8082992754c4d2c93199077669ab085","0d247affe5fc401fa68e2682032d8a5a","2f155642b4c241b6ba1776ae6de425b6","f88425bfcd204aed952526f56843f06f","4e509895258245dea3de16c4f47e4f3b","189dceb6b3dc47aba13c8614a36f87b2","acc7b578812348059780bf199633b8df","b0a8962084804f40a2d74855cc3bb5e8","19b627703293417d9b5085dd5a508866","5943e00389304257b29c27e0a425c929","767d93dbb96845ec9b3c5074f468f261","edda198d6c2647d5967d6d4e6919eb8b","0378235de78a495c8781f6cb299d2094","d6308eeb79c74ce58686e305a3109256","036e2236edb44a5bbb48bca0d517e044","5b7a2817c9f64c7b8765ae67a2e1933d","0801736cbcb24ca6a1e92822b908fcd6","d970834854be41998125718cf1567d5c","9b44851383ca434eba7f41d232e82e31","248855b3a77c40a58e4a924aaee8d22a","e0c3b516c933473cb1e0b23f2643c7a3","d0d88d4a1e5c45c1a4cb7c767d5b761e","1ca1976a99f4464f8e17e32cb7a08093","d45dca9d9f174044b9a3a36d883e0f61","abda310c836143bd896a43e5a7dc16c1","ae56a0f62397484a9f0e678c69fa674b","25aad33a1e2f48ff99aa3fcc0ca50a8c","cd48f56ab40c473e88aae035d0a56d87","e4a9a004afc54252b56e7719ea60f9b4","d6bcd8eb5bc24396a44f6699e576c7ba","6aabdbcc9d7b4b759b28e1fce324ea7b","628a82cbe3c6479392ae3caff6867467","a406c2db19e24dd2960e806e4eaaa510","b37b47c40096494b992e7bafd3692d03"]},"executionInfo":{"elapsed":93802,"status":"ok","timestamp":1651541996608,"user":{"displayName":"Michael Tang","userId":"18009171711496870090"},"user_tz":240},"id":"de5fbda0","outputId":"54496ef0-1455-44c8-a04c-da1b065c1637","scrolled":true},"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading builder script:   0%|          | 0.00/2.03k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c1f56010394a4de6ad0d6606512961bf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading metadata:   0%|          | 0.00/1.25k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b4c2854f450944c3ac4e1e2e2c8c52fc"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Downloading and preparing dataset wikitext/wikitext-103-v1 (download: 181.42 MiB, generated: 522.23 MiB, post-processed: Unknown size, total: 703.64 MiB) to /root/.cache/huggingface/datasets/wikitext/wikitext-103-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126...\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading data:   0%|          | 0.00/190M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f7ab22541968478f85b6dc94f766ad95"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d247affe5fc401fa68e2682032d8a5a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Generating train split:   0%|          | 0/1801350 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0378235de78a495c8781f6cb299d2094"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d45dca9d9f174044b9a3a36d883e0f61"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Dataset wikitext downloaded and prepared to /root/.cache/huggingface/datasets/wikitext/wikitext-103-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126. Subsequent calls will reuse this data.\n"]}],"source":["data = loadData(train=True)"],"id":"de5fbda0"},{"cell_type":"markdown","metadata":{"id":"48ec35e7"},"source":["testing: compared tree pre-order traversal of 10 documents with raw data. No bug. "],"id":"48ec35e7"},{"cell_type":"markdown","metadata":{"id":"e49e36c2"},"source":["##### Training set statistics"],"id":"e49e36c2"},{"cell_type":"markdown","metadata":{"id":"szkFxU2gg_zd"},"source":["Include stats of  \n","- number of paragraphs per article  \n","- average length of paragraphs per article \n","- maximum depth of articles  "],"id":"szkFxU2gg_zd"},{"cell_type":"code","execution_count":9,"metadata":{"id":"abf1a160","executionInfo":{"status":"ok","timestamp":1651541053426,"user_tz":240,"elapsed":39,"user":{"displayName":"Michael Tang","userId":"18009171711496870090"}}},"outputs":[],"source":["from matplotlib import pyplot as plt \n","import numpy as np\n","import scipy.stats as stats\n","def getStat(data):\n","    # number of paragraphs per article \n","    num_para = np.array([len(x['paragraphs']) for x in data])\n","    counts, edges, bars = plt.hist(num_para,40)\n","    print(\"========= number of paragraphs per article========\")\n","    print(pd.Series(num_para).describe())\n","    plt.show()\n","    \n","    # number of sentences per paragraph \n","    para_lens = []\n","    for d in data:\n","        paras = d['paragraphs']\n","        for para in paras:\n","            lgh = len(para.split('.'))-1\n","            para_lens.append(lgh)\n","    print(\"========= number of sentences per paragraph========\")\n","    print(pd.Series(para_lens).describe())\n","    _,_,_ = plt.hist(para_lens,40)\n","    plt.show()\n","\n","    \n","    # depth of articles \n","    depths = [d['tree'].depth for d in data]\n","    print(\"========= maximum depth of articles========\")\n","    print(pd.Series(depths).describe())\n","    _,_,_ = plt.hist(depths)\n","    plt.show()\n","\n","    return \n","    "],"id":"abf1a160"},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"elapsed":2024,"status":"ok","timestamp":1651541055413,"user":{"displayName":"Michael Tang","userId":"18009171711496870090"},"user_tz":240},"id":"b1e3193c","outputId":"23144d35-7339-4970-81ae-3118b5369513"},"outputs":[{"output_type":"stream","name":"stdout","text":["========= number of paragraphs per article========\n","count    29444.000000\n","mean        29.206052\n","std         25.829840\n","min          1.000000\n","25%         12.000000\n","50%         21.000000\n","75%         37.000000\n","max        284.000000\n","dtype: float64\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}},{"output_type":"stream","name":"stdout","text":["========= number of sentences per paragraph========\n","count    859943.000000\n","mean          4.830331\n","std           3.650685\n","min           0.000000\n","25%           2.000000\n","50%           4.000000\n","75%           7.000000\n","max          75.000000\n","dtype: float64\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}},{"output_type":"stream","name":"stdout","text":["========= maximum depth of articles========\n","count    29444.000000\n","mean         1.758355\n","std          0.686543\n","min          0.000000\n","25%          1.000000\n","50%          2.000000\n","75%          2.000000\n","max          5.000000\n","dtype: float64\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}],"source":["getStat(data)"],"id":"b1e3193c"},{"cell_type":"markdown","metadata":{"id":"FSKtCnA_BdMD"},"source":["##### Preparing SimCSE + Doc2Vec + contrastive paragraph embeddings + related setup (please run!)"],"id":"FSKtCnA_BdMD"},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":979,"status":"ok","timestamp":1651542021691,"user":{"displayName":"Michael Tang","userId":"18009171711496870090"},"user_tz":240},"id":"Qkh6gZxswMUB"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n","import random\n","random.seed(10)\n","torch.manual_seed(0)\n","np.random.seed(0)\n","import re\n","import time\n","import math\n","from IPython.utils import io"],"id":"Qkh6gZxswMUB"},{"cell_type":"code","source":["import transformers\n","from transformers import AutoModel, AutoTokenizer,BertTokenizer\n","from transformers import AutoConfig"],"metadata":{"id":"PNuKzW9BDz-D","executionInfo":{"status":"ok","timestamp":1651542022207,"user_tz":240,"elapsed":14,"user":{"displayName":"Michael Tang","userId":"18009171711496870090"}}},"id":"PNuKzW9BDz-D","execution_count":15,"outputs":[]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1651542022208,"user":{"displayName":"Michael Tang","userId":"18009171711496870090"},"user_tz":240},"id":"87baa622"},"outputs":[],"source":["# %%capture\n","# !pip install simcse"],"id":"87baa622"},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1651542022208,"user":{"displayName":"Michael Tang","userId":"18009171711496870090"},"user_tz":240},"id":"szpU2OFCB3s0"},"outputs":[],"source":["# from simcse import SimCSE\n","# simcse_model = SimCSE(\"princeton-nlp/sup-simcse-bert-base-uncased\")"],"id":"szpU2OFCB3s0"},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1651542022208,"user":{"displayName":"Michael Tang","userId":"18009171711496870090"},"user_tz":240},"id":"mP43IO_uB3wn"},"outputs":[],"source":["# # mini-demo with print suppression\n","# with io.capture_output() as captured:\n","#     print('this should not be printed')\n","#     embeddings = simcse_model.encode(\"A woman is reading, and a man is here\")\n","# print('this should be printed')\n","# print(embeddings.shape)"],"id":"mP43IO_uB3wn"},{"cell_type":"code","execution_count":19,"metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1651542022209,"user":{"displayName":"Michael Tang","userId":"18009171711496870090"},"user_tz":240},"id":"zmqyWpNGmruU"},"outputs":[],"source":["SIMCSE_DIM = 768\n","MAX_DEPTH = 8\n","# MAX_PARAS = 300\n","# MAX_SENTS_P/ER_PARA = 80"],"id":"zmqyWpNGmruU"},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1651542022209,"user":{"displayName":"Michael Tang","userId":"18009171711496870090"},"user_tz":240},"id":"ThJG0MPafOBW"},"outputs":[],"source":["# for segmentation\n","START_IDX = 0\n","END_IDX = MAX_DEPTH"],"id":"ThJG0MPafOBW"},{"cell_type":"code","execution_count":21,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1651542022209,"user":{"displayName":"Michael Tang","userId":"18009171711496870090"},"user_tz":240},"id":"0NiXOdkJHcI5"},"outputs":[],"source":["# !pip install gensim==4.1.2\n","# !pip install cython\n","# !pip install nltk\n","\n","# import nltk\n","# nltk.download('punkt')\n","# from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n","# from nltk.tokenize import word_tokenize"],"id":"0NiXOdkJHcI5"},{"cell_type":"code","source":[""],"metadata":{"id":"KYiSChxkDkpt","executionInfo":{"status":"ok","timestamp":1651542022210,"user_tz":240,"elapsed":13,"user":{"displayName":"Michael Tang","userId":"18009171711496870090"}}},"id":"KYiSChxkDkpt","execution_count":21,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xBFAl4oVXTj5"},"source":["Train Doc2Vec Encoder (only done once, after that we load from 'd2v.model')"],"id":"xBFAl4oVXTj5"},{"cell_type":"code","execution_count":22,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1651542022210,"user":{"displayName":"Michael Tang","userId":"18009171711496870090"},"user_tz":240},"id":"zb6rnfyOH7jv"},"outputs":[],"source":["# # DONT NEED TO RUN THIS CELL IF U HAVE THE FILES\n","# documents = [d['paragraphs'] for d in data]\n","# paragraphs = []\n","# for doc in documents:\n","#   for p in doc:\n","#     paragraphs.append(p)\n","# # print(len(paragraphs))\n","# tagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(paragraphs)]\n","\n","\n","# max_epochs = 15\n","# vec_size = 128\n","# alpha = 0.025\n","\n","# with io.capture_output() as captured:\n","#     doc2vec_model = Doc2Vec(vector_size=128, min_count=2, window=2, workers=4, dm=1, alpha=alpha, min_alpha=0.00025)\n","#     doc2vec_model.build_vocab(tagged_data)\n","\n","# for epoch in range(max_epochs):\n","#     print('iteration {0}'.format(epoch))\n","#     with io.capture_output() as captured:\n","#         doc2vec_model.train(tagged_data,\n","#                             total_examples=doc2vec_model.corpus_count,\n","#                             epochs=4)\n","#     # decrease the learning rate\n","#     doc2vec_model.alpha -= 0.0002\n","#     # fix the learning rate, no decay\n","#     doc2vec_model.min_alpha = doc2vec_model.alpha\n","\n","# doc2vec_model.save(\"d2v.model\")\n"],"id":"zb6rnfyOH7jv"},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":29135,"status":"ok","timestamp":1651542065255,"user":{"displayName":"Michael Tang","userId":"18009171711496870090"},"user_tz":240},"id":"ANF-NCtImfWz","outputId":"2e3baf7a-14ed-4a71-df5e-f954c35d367c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"id":"ANF-NCtImfWz"},{"cell_type":"markdown","metadata":{"id":"d9GjBog-n4k4"},"source":["Make sure to add 484-finalProject as shortcut in drive."],"id":"d9GjBog-n4k4"},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1651542065255,"user":{"displayName":"Michael Tang","userId":"18009171711496870090"},"user_tz":240},"id":"jxT9-NNFmvTL","outputId":"fe405314-ec6a-41a5-9ab7-b961ac5f0ab2"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/.shortcut-targets-by-id/154EdVBqpeIxqbeun-wGvxSgsYCVV1wdV/484-finalProject\n"]}],"source":["cd /content/drive/My\\ Drive/484-finalProject"],"id":"jxT9-NNFmvTL"},{"cell_type":"markdown","source":["Para2Vec"],"metadata":{"id":"boDox1wADt55"},"id":"boDox1wADt55"},{"cell_type":"code","source":["import os\n","import json\n","def load_Para2Vec(path='./para2vec-simcse-bert-base-uncased/'):\n","    state_dict = torch.load(os.path.join(path, \"pytorch_model.bin\"), map_location=torch.device(\"cpu\"))\n","    new_state_dict = {}\n","    for key, param in state_dict.items():\n","        # Replace \"mlp\" to \"pooler\"\n","        if \"mlp\" in key:\n","            key = key.replace(\"mlp\", \"pooler\")\n","\n","        # Delete \"bert\" or \"roberta\" prefix\n","        if \"bert.\" in key:\n","            key = key.replace(\"bert.\", \"\")\n","        if \"roberta.\" in key:\n","            key = key.replace(\"roberta.\", \"\")\n","\n","        new_state_dict[key] = param\n","    config = json.load(open(os.path.join(path, \"config.json\")))\n","    for i in range(len(config[\"architectures\"])):\n","        config[\"architectures\"][i] = config[\"architectures\"][i].replace(\"ForCL\", \"Model\")\n","    json.dump(config, open(os.path.join(path, \"config.json\"), \"w\"), indent=2)\n","      \n","    return \n","load_Para2Vec()"],"metadata":{"id":"QVyVu4-k23QT","executionInfo":{"status":"ok","timestamp":1651542072840,"user_tz":240,"elapsed":7591,"user":{"displayName":"Michael Tang","userId":"18009171711496870090"}}},"id":"QVyVu4-k23QT","execution_count":26,"outputs":[]},{"cell_type":"code","source":["para2vec_model = AutoModel.from_pretrained('./para2vec-simcse-bert-base-uncased/',config=\"./para2vec-simcse-bert-base-uncased/tokenizer_config.json\")\n","tokenizer = AutoTokenizer.from_pretrained(\"princeton-nlp/sup-simcse-bert-base-uncased\",config=\"./para2vec-simcse-bert-base-uncased/tokenizer_config.json\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0,"referenced_widgets":["3312bfd1a1354cb4ad1169f952863a9e","738cd92cb0874a9eb2627d767274762f","81f0915d99594d609402266f3d8b7940","619f1102c3884f93bbd65efb3cfe70d9","fabb55919e3d4d788e048fe7cc08ef38","1121e5de3d0942b3aaf821c2c8631de2","26266ccec866411a90b585d53baa015f","2bd8a237668440c48edb17354f64385c","8344ebc1e9e24d5d99185dc9ee744e18","f54e3cb459df491fa4fad4ba4aa494b8","a45dad838bc34b03be1f47c5cf9cbea2","9e2c6925d8084d69ab6464512f3617ec","52244325f04845fda0497de676feab1b","aba22c48df5c485383677b9e34a8ef35","f70835726cf744adbbed05f682ffcf0c","560accc171c54ab1bb9b2408bc17bc47","e268d1d5bc3d4fcb9f7ff2aae8a8e4ee","00507c26807c4a1f88b55de07a4795ae","18325020c20a4d358cc60f5bb26965da","4a520d9088894fdda9f254b71e933021","716671997a6b43c7a1b7bd595aebca33","38ee3c7e13cb40cd982d352d61bc777f","3912b4fce23a412f855fac4ca43d5750","f710e031aebc4973a603ef68c8bdb47d","abc37f6c8f1b4803b076beb1d62308a3","93478367bb634adb82cd7bafb37a2dbb","4d198cd5c2064557bc284bcb96da460a","167dad9f5ab34bd39e170cc389bcff6b","5493b4cd010744d49f5dea5d87033367","1bb567fd545a4e0b8ea4978893639624","ffdd33a86db3479e803d99cb157b9637","a953c98f007e417a90109b26d50863ac","f493ae3308a5476d97fbadf0949311f2","3de100c6cfce435899872c3cad59c5cf","ddfc7d600ae64543857f85b936608828","8817f4ca8a4b4109b487e3ebf7fe0e0d","e97d653bdcb047d9aa7dc129a1b244d8","855b097c9565486b88a66a3aea0d8206","4a4ab8e7953243a6830c483fde47f6ec","c08b4fd096b341409555ca8e8d5470e3","f2c2596c03a64341b2a4ef98e6cd3cf8","928d565279d0416e85c9a8ce3a88c3d4","30c3721218aa4c368d51e7d6f53a3153","abf3ed128a77443ba38b51b74249661a"]},"id":"RFrcXdcDED5u","executionInfo":{"status":"ok","timestamp":1651542087595,"user_tz":240,"elapsed":14763,"user":{"displayName":"Michael Tang","userId":"18009171711496870090"}},"outputId":"936b0e4d-ab7a-45ac-eb0e-1629c325d551"},"id":"RFrcXdcDED5u","execution_count":27,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/252 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3312bfd1a1354cb4ad1169f952863a9e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/689 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e2c6925d8084d69ab6464512f3617ec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3912b4fce23a412f855fac4ca43d5750"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3de100c6cfce435899872c3cad59c5cf"}},"metadata":{}}]},{"cell_type":"code","execution_count":28,"metadata":{"executionInfo":{"elapsed":21,"status":"ok","timestamp":1651542087596,"user":{"displayName":"Michael Tang","userId":"18009171711496870090"},"user_tz":240},"id":"04k_YJCVBigD"},"outputs":[],"source":["# doc2vec_model= Doc2Vec.load(\"models/256_d2v.model\")"],"id":"04k_YJCVBigD"},{"cell_type":"code","execution_count":29,"metadata":{"executionInfo":{"elapsed":23,"status":"ok","timestamp":1651542087599,"user":{"displayName":"Michael Tang","userId":"18009171711496870090"},"user_tz":240},"id":"E12J-l2eXiM-"},"outputs":[],"source":["# # Experiment with inference and lookups :)\n","# test1 = word_tokenize(\"This is a test of the paragraph embedding model.\".lower())\n","# test2 = word_tokenize(\"I am testing my model to encode paragraphs.\".lower())\n","# test3 = word_tokenize(\"Complete gibberish, baby talk with a model. Goo goo ga ga.\")\n","# test4 = word_tokenize(\"This is a test for my model to embed paragraphs.\".lower())\n","\n","# v = doc2vec_model.infer_vector(test1)\n","# v1 = doc2vec_model.infer_vector(test1)\n","# v2 = doc2vec_model.infer_vector(test2)\n","# v3 = doc2vec_model.infer_vector(test3)\n","# v4 = doc2vec_model.infer_vector(test4)\n","# print(np.linalg.norm(v1 - v))  # normal inference, turns tokenized, lowercase paragraph into 128-vector\n","# print(np.linalg.norm(v1 - v2)) \n","# print(np.linalg.norm(v1 - v3))\n","# print(np.linalg.norm(v1 - v4))\n","# print()\n","\n","# print(np.dot(v1, v)/(np.linalg.norm(v1)*np.linalg.norm(v)))  # normal inference, turns tokenized, lowercase paragraph into 128-vector\n","# print(np.dot(v1, v2)/(np.linalg.norm(v1)*np.linalg.norm(v2)))\n","# print(np.dot(v1, v3)/(np.linalg.norm(v1)*np.linalg.norm(v3)))\n","# print(np.dot(v1, v4)/(np.linalg.norm(v1)*np.linalg.norm(v4)))\n","\n","# print()\n","# print(doc2vec_model.similarity_unseen_docs(test, test4))\n","\n","# # print(doc2vec_model.dv['2'])  # dictionary where each paragraph embedding has a numbered id"],"id":"E12J-l2eXiM-"},{"cell_type":"code","execution_count":30,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1651542088151,"user":{"displayName":"Michael Tang","userId":"18009171711496870090"},"user_tz":240},"id":"1byrFPm5do8S"},"outputs":[],"source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"],"id":"1byrFPm5do8S"},{"cell_type":"markdown","metadata":{"id":"PDWF2apsjZT1"},"source":["##### Preparing LCA loss evaluation function + tools for trees (please run!)"],"id":"PDWF2apsjZT1"},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":57,"status":"ok","timestamp":1651541996609,"user":{"displayName":"Michael Tang","userId":"18009171711496870090"},"user_tz":240},"id":"IBXXXBS4jjcT"},"outputs":[],"source":["# tree-related helper functions\n","\n","# iterate over a tree rooted at node in preorder traversal\n","def preorder(node):\n","    if len(node.children) == 0:\n","        yield node\n","    for ch in node.children:\n","        yield from preorder(ch)\n","\n","# only prints leaves, i.e. text representations of paragraph\n","# note: depends on accurate text, level population\n","# text should be indices\n","def print_tree(curNode):\n","    if curNode.level == -1:\n","        print(curNode.text, end='')\n","        return\n","    print('[', end='')\n","    for idx, child in enumerate(curNode.children):\n","        print_tree(child)\n","        if idx < len(curNode.children) - 1:\n","            print(', ', end='')\n","    print(']', end='')\n","    if curNode.level == 0:\n","        print() # final print after entire tree is printed\n","\n","# text should be snippets\n","def print_snippet_tree(curNode, indent='  '):\n","    if curNode.level == -1:\n","        print(indent + curNode.text, end='')\n","        return\n","    print(indent+'[heading]')\n","    for idx, child in enumerate(curNode.children):\n","        print_snippet_tree(child, indent+'  ')\n","        if idx < len(curNode.children) - 1:\n","            print()\n","    if curNode.level == 0:\n","        print() # final print after entire tree is printed\n","\n","def clone_tree(root):\n","    root_copy = Node(root.text, root.level)\n","    for ch in root.children:\n","        root_copy.insertChild(clone_tree(ch))\n","        root_copy.children[-1].linkParent(root_copy)\n","    return root_copy\n","\n","# return indexified tree with text as paragraphs to text as indices of paragraphs for more concise printing\n","def indexified_tree(root):\n","    root_copy = clone_tree(root)\n","    for idx, node in enumerate(preorder(root_copy)):\n","        node.text = idx\n","    return root_copy\n","\n","# return indexified tree with text as paragraphs to text as indices of paragraphs for more concise printing\n","def textified_tree(root, paras):\n","    root_copy = clone_tree(root)\n","    for idx, node in enumerate(preorder(root_copy)):\n","        node.text = paras[idx][:40] + '...'\n","    return root_copy\n","\n","# print_tree(roots[0])\n","# print_tree(train_y[0].root)"],"id":"IBXXXBS4jjcT"},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":55,"status":"ok","timestamp":1651541996610,"user":{"displayName":"Michael Tang","userId":"18009171711496870090"},"user_tz":240},"id":"z9hZqeHXjef7"},"outputs":[],"source":["# lca-related helper functions and lca loss\n","# note: assumed indexified trees\n","def trace_helper(node, i, trace):\n","    if node.text == i:\n","        return True\n","    for idx, ch in enumerate(node.children):\n","        if trace_helper(ch, i, trace):\n","            trace.append(idx)\n","            return True\n","    return False\n","\n","def get_trace(root, i):\n","    trace = []\n","    trace_helper(root, i, trace)\n","    trace.reverse()\n","    return trace\n","\n","def compute_lca_dist(root, i, j):\n","    trace_i = get_trace(root, i)\n","    trace_j = get_trace(root, j)\n","    # print(trace_i)\n","    # print(trace_j)\n","    for idx in range(min(len(trace_i), len(trace_j))):\n","        if trace_i[idx] != trace_j[idx]:\n","            return len(trace_i) + len(trace_j) - 2 * idx\n","    return len(trace_i) + len(trace_j)\n","\n","def lca_loss(root1, root2, num_paras):\n","    loss = 0\n","    for i in range(2, num_paras+1): # 1-indexed from indexify\n","    # for j in range(i+1, num_paras+1):\n","        j=i-1\n","        dist1 = compute_lca_dist(root1, i, j)\n","        dist2 = compute_lca_dist(root2, i, j)\n","        # print(i, j, dist1, dist2)\n","        loss += (dist1 - dist2) * (dist1 - dist2)\n","    if num_paras == 1:\n","        return loss\n","    return loss /(num_paras - 1)\n","\n","def batch_lca_loss(roots1, roots2, num_paras):\n","    tt = 0\n","    for root1, root2, num in zip(roots1, roots2, num_paras):\n","        tt += lca_loss(root1, root2, num)\n","    return tt / len(roots1)"],"id":"z9hZqeHXjef7"},{"cell_type":"markdown","metadata":{"id":"X_aNj3_bh9VG"},"source":["##### Model 1: End-to-end -- note this code may be behind the local version on Evan's big boi computer"],"id":"X_aNj3_bh9VG"},{"cell_type":"markdown","metadata":{"id":"2FDvqg_Xi2mR"},"source":["Approach: SimCSE/Doc2Vec + bidirectional LSTM + transformer -> segmentations"],"id":"2FDvqg_Xi2mR"},{"cell_type":"code","execution_count":null,"metadata":{"id":"OatKuhkFCEcQ","colab":{"base_uri":"https://localhost:8080/","height":365},"executionInfo":{"status":"error","timestamp":1651535705870,"user_tz":240,"elapsed":1291,"user":{"displayName":"Jiatong Yu","userId":"17536159758394002445"}},"outputId":"fa2973a6-538c-4717-8edf-ef374b016918"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-19-e91168c8fc01>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mHeaderNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     def __init__(self, hid_dim=32, n_layers=1, num_heads=4, # CURRENTLY ONLY SUPPROTS N_LAYERS=1\n\u001b[1;32m      3\u001b[0m                  num_enc_layers=1, num_dec_layers=1, ff_dim=1024, dropout=0.1, output_dim=2*MAX_DEPTH):\n\u001b[1;32m      4\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-19-e91168c8fc01>\u001b[0m in \u001b[0;36mHeaderNet\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mHeaderNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     def __init__(self, hid_dim=32, n_layers=1, num_heads=4, # CURRENTLY ONLY SUPPROTS N_LAYERS=1\n\u001b[0;32m----> 3\u001b[0;31m                  num_enc_layers=1, num_dec_layers=1, ff_dim=1024, dropout=0.1, output_dim=2*MAX_DEPTH):\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'MAX_DEPTH' is not defined"]}],"source":["class HeaderNet(nn.Module):\n","    def __init__(self, hid_dim=32, n_layers=1, num_heads=4, # CURRENTLY ONLY SUPPROTS N_LAYERS=1\n","                 num_enc_layers=1, num_dec_layers=1, ff_dim=1024, dropout=0.1, output_dim=2*MAX_DEPTH):\n","        super().__init__()\n","\n","        self.hid_dim = hid_dim\n","        self.n_layers = n_layers\n","        self.num_heads = num_heads\n","        self.num_enc_layers = num_enc_layers\n","        self.num_dec_layers = num_dec_layers\n","        self.ff_dim = ff_dim\n","        self.dropout = dropout\n","\n","        self.lstm = nn.LSTM(SIMCSE_DIM,\n","                            hid_dim,\n","                            num_layers=n_layers,\n","                            bidirectional=True,\n","                            batch_first=True)\n","        self.transformer = nn.Transformer(d_model=hid_dim*4, nhead=num_heads, num_encoder_layers=num_enc_layers,\n","                                          num_decoder_layers=num_dec_layers, dim_feedforward=ff_dim, dropout=dropout, batch_first=True)\n","        self.linear = nn.Linear(hid_dim*4, output_dim)\n","\n","    # masks for transformer to account for variable numbers of paragraphs per article\n","    def compute_len_masks(self, batch):\n","        lens = [len(text) for text in batch]\n","        max_num_paras = max(lens)\n","        len_masks = torch.zeros(len(batch), max_num_paras)\n","        for idx, text_len in enumerate(lens):\n","            len_masks[idx, text_len:] = 1\n","        return lens, len_masks.to(self.device)\n","\n","    # compute sentence embeddings from paragraphs in text\n","    def compute_sentence_embeddings(self, batch):\n","        num_para = [len(x) for x in batch]\n","        batch_para = []\n","\n","        # build list of lists of sentence embeddings\n","        # note that num paragraphs and num sentences are both variable\n","        sents_emb = []\n","        for text in batch:\n","            for para in text:\n","                para = re.sub('\\n', '', para)\n","                sents = re.split('[.]|[!]|[?]', para.strip())\n","                with io.capture_output() as captured:\n","                    sents_emb_i = simcse_model.encode(sents, device=self.device, batch_size=len(sents))\n","                sents_emb.append(sents_emb_i)\n","        return sents_emb\n","\n","    # returns a list of slices along the first dim according to a given list of slice lengths\n","    def partition(self, data, lens):\n","        data_list = []\n","        idx = 0\n","        for len_i in lens:\n","            data_list.append(data[idx:idx+len_i])\n","            idx += len_i\n","        return data_list\n","\n","    def forward(self, batch):\n","        # compute len masks\n","        lens, len_masks = self.compute_len_masks(batch)\n","\n","        # # paragraphs -> sentence embeddings\n","        # sents_emb = self.compute_sentence_embeddings(batch)\n","        # sent_lens = [x.shape[0] for x in sents_emb]\n","        # sents_emb = torch.nn.utils.rnn.pad_sequence(sents_emb, batch_first=True).to(device)\n","        # packed_in = torch.nn.utils.rnn.pack_padded_sequence(sents_emb, sent_lens, batch_first=True, enforce_sorted=False)\n","\n","        # # sentence embeddings -> paragraph embeddings\n","        # packed_out, (hidden, cell) = self.lstm(packed_in.to(self.device))\n","        # para_emb = torch.cat((hidden[0], cell[0], hidden[1], cell[1]), dim=1)\n","        # para_list = self.partition(para_emb, lens) # group paragraph embeddings by article\n","        # print(len(para_list))\n","        # print(para_list[0].shape)\n","        # text_emb = torch.nn.utils.rnn.pad_sequence(para_list, batch_first=True).to(device)\n","        \n","        \n","        para_list = []\n","        for article in batch:\n","          para_list.append(torch.stack([torch.tensor(doc2vec_model.infer_vector(word_tokenize(p.lower())), device=device) for p in article], dim=0))\n","        text_emb = torch.nn.utils.rnn.pad_sequence(para_list, batch_first=True).to(device)\n","        \n","        # paragraph embeddings -> outlines\n","        logits = self.transformer(text_emb, text_emb, src_key_padding_mask=len_masks, tgt_key_padding_mask=len_masks)\n","        logits = logits[len_masks == 0]\n","        logits = self.linear(logits)\n","        logit_list = self.partition(logits, lens)\n","        \n","        return logit_list\n","\n","    @property\n","    def device(self) -> torch.device:\n","        \"\"\" Determine which device to place the Tensors upon, CPU or GPU.\n","        \"\"\"\n","        return device\n","    \n","    @staticmethod\n","    def load(model_path: str):\n","        \"\"\" Load the model from a file.\n","        @param model_path (str): path to model\n","        \"\"\"\n","        params = torch.load(model_path, map_location=lambda storage, loc: storage)\n","        args = params['args']\n","        model = HeaderNet(**args)\n","        model.load_state_dict(params['state_dict'])\n","\n","        return model\n","\n","    def save(self, path: str):\n","        \"\"\" Save the model to a file.\n","        @param path (str): path to the model\n","        \"\"\"\n","\n","        params = {\n","            'args': dict(hid_dim=self.hid_dim, n_layers=self.n_layers, num_heads=self.num_heads,\n","                 num_enc_layers=self.num_enc_layers, num_dec_layers=self.num_dec_layers, ff_dim=self.ff_dim, dropout=self.dropout),\n","            'state_dict': self.state_dict()\n","        }\n","\n","        torch.save(params, path)"],"id":"OatKuhkFCEcQ"},{"cell_type":"code","execution_count":null,"metadata":{"id":"_qnZNERGsYn_"},"outputs":[],"source":["# # encode just the depths -- note that this is insufficient to reconstruct the tree\n","# def add_tree_depths(node, depths, cur_depth):\n","#     if node.level == -1: # this is paragraph node, not a heading\n","#         depths.append(cur_depth)\n","#         return\n","#     for child in node.children:\n","#         add_tree_depths(child, depths, cur_depth+1)\n","\n","# def get_depths(trees, one_hot=True):\n","#     depths = []\n","#     for tree in trees:\n","#         depths_i = []\n","#         add_tree_depths(tree.root, depths_i, 0)\n","#         if one_hot:\n","#             depths.append(F.one_hot(torch.tensor(depths_i), num_classes=MAX_DEPTH))\n","#         else:\n","#             depths.append(torch.tensor(depths_i))\n","#     return torch.stack(depths, dim=0)\n","\n","# encode outlines\n","# note: we use 0 as padding symbol\n","def add_tree_outlines(node, outlines, cur_outline):\n","    if node.level == -1: # this is paragraph node, not a heading\n","        outlines.append(torch.clone(cur_outline))\n","        return\n","    for child in node.children:\n","        cur_outline[node.level] += 1\n","        add_tree_outlines(child, outlines, cur_outline)\n","    cur_outline[node.level] = 0\n","\n","def get_outlines(paras, trees):\n","    max_paras = max([len(x) for x in paras])\n","    outlines = []\n","    for tree in trees:\n","        outlines_i = []\n","        add_tree_outlines(tree.root, outlines_i, torch.zeros(MAX_DEPTH))\n","        outlines_i = torch.stack(outlines_i, dim=0).to(device)\n","        outlines.append(outlines_i)\n","    return outlines\n","\n","# encode breaks\n","\n","def add_tree_segs(node, outlines, cur_outline):\n","    if node.level == -1: # this is paragraph node, not a heading\n","        outlines.append(torch.clone(cur_outline))\n","        return\n","    for child in node.children:\n","        if child == node.children[0]: # first entry\n","            cur_outline[START_IDX + node.level] = 1\n","        if child == node.children[-1]:\n","            cur_outline[END_IDX + node.level] = 1\n","        add_tree_segs(child, outlines, cur_outline)\n","        if child == node.children[0]:\n","            cur_outline[START_IDX + node.level] = 0\n","        if child == node.children[-1]:\n","            cur_outline[END_IDX + node.level] = 0\n","    cur_outline[node.level+1] = 0\n","\n","def get_segs(paras, trees):\n","    max_paras = max([len(x) for x in paras])\n","    outlines = []\n","    for tree in trees:\n","        outlines_i = []\n","        add_tree_segs(tree.root, outlines_i, torch.zeros(2*MAX_DEPTH))\n","        outlines_i = torch.stack(outlines_i, dim=0).to(device)\n","        outlines.append(outlines_i)\n","    return outlines"],"id":"_qnZNERGsYn_"},{"cell_type":"code","execution_count":null,"metadata":{"id":"eyhj2RhkGsXl"},"outputs":[],"source":["# test demo\n","# model = HeaderNet()\n","# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","# print('use device: %s' % device)\n","# model = model.to(device)\n","# y_hat = model(train_X[0:5])\n","# y = get_outlines(train_X[0:5], train_y[0:5])"],"id":"eyhj2RhkGsXl"},{"cell_type":"code","execution_count":31,"metadata":{"id":"QH-fMeW-oydJ","executionInfo":{"status":"ok","timestamp":1651542123788,"user_tz":240,"elapsed":312,"user":{"displayName":"Michael Tang","userId":"18009171711496870090"}}},"outputs":[],"source":["def get_paragraphs_trees(data):\n","    paras = [d['paragraphs'] for d in data]\n","    trees = [d['tree'] for d in data]\n","    return paras, trees\n","\n","def get_split_data(data, train_ratio=0.05, val_ratio=0.001, test_ratio=0.002):\n","    random.shuffle(data)\n","    n_data = len(data)\n","    train_idx = int(train_ratio*n_data)\n","    val_idx = int((train_ratio + val_ratio)*n_data)\n","    test_idx = int((train_ratio + val_ratio + test_ratio)*n_data)\n","    train_X, train_y = get_paragraphs_trees(data[:train_idx])\n","    val_X, val_y = get_paragraphs_trees(data[train_idx:val_idx])\n","    test_X, test_y = get_paragraphs_trees(data[val_idx:test_idx])\n","    return train_X, train_y, val_X, val_y, test_X, test_y"],"id":"QH-fMeW-oydJ"},{"cell_type":"code","execution_count":32,"metadata":{"id":"2RKkK93gqanM","executionInfo":{"status":"ok","timestamp":1651542123789,"user_tz":240,"elapsed":4,"user":{"displayName":"Michael Tang","userId":"18009171711496870090"}}},"outputs":[],"source":["train_X, train_y, val_X, val_y, test_X, test_y = get_split_data(data)"],"id":"2RKkK93gqanM"},{"cell_type":"code","execution_count":null,"metadata":{"id":"YGLH3oYgIUoC"},"outputs":[],"source":["train_reshaped = torch.cat(get_segs(train_X, train_y), dim=0)\n","train_mean = torch.mean(train_reshaped, dim=0)\n","train_std = torch.std(train_reshaped, dim=0)\n","train_std += 1e-6 # deal with div by zero\n","def normalize(segs):\n","    normed_segs = []\n","    for s in segs:\n","        normed_segs.append(torch.div(torch.sub(s, train_mean), train_std))\n","    return normed_segs"],"id":"YGLH3oYgIUoC"},{"cell_type":"code","execution_count":null,"metadata":{"id":"x8J51HNFJFjG"},"outputs":[],"source":["# # testing normalize\n","# segs = get_segs(train_X, train_y)\n","# normed_segs = normalize(get_segs(train_X, train_y))\n","\n","# v_segs = get_segs(val_X, val_y)\n","# v_normed_segs = normalize(get_segs(val_X, val_y))\n","\n","# print(torch.mean(torch.cat(segs),dim=0))\n","# print(torch.mean(torch.cat(normed_segs),dim=0))\n","# print(torch.std(torch.cat(segs),dim=0))\n","# print(torch.std(torch.cat(normed_segs),dim=0))\n","\n","# print(torch.mean(torch.cat(v_segs),dim=0))\n","# print(torch.mean(torch.cat(v_normed_segs),dim=0))\n","# print(torch.std(torch.cat(v_segs),dim=0))\n","# print(torch.std(torch.cat(v_normed_segs),dim=0))"],"id":"x8J51HNFJFjG"},{"cell_type":"code","execution_count":null,"metadata":{"id":"CZZo91Uglvx_"},"outputs":[],"source":["# iterate over batches of data and labels\n","# articles in batch are sorted by descending num paragraphs for efficient packing/LSTM inference\n","def batch_iter(data, labels, batch_size, shuffle=False):\n","    batch_num = math.ceil(len(data) / batch_size)\n","    index_array = list(range(len(data)))\n","\n","    if shuffle:\n","        np.random.shuffle(index_array)\n","\n","    for i in range(batch_num):\n","        indices = index_array[i * batch_size: (i + 1) * batch_size]\n","        indices = sorted(indices, key=lambda idx: len(data[idx]), reverse=True)\n","        batch_data = [data[idx] for idx in indices]\n","        batch_labels = [labels[idx] for idx in indices]\n","\n","        yield batch_data, batch_labels"],"id":"CZZo91Uglvx_"},{"cell_type":"code","execution_count":null,"metadata":{"id":"6zQERFHtdJxK"},"outputs":[],"source":["def outline_loss(y_hat, y, output_format='segmentation'):\n","    tt_loss = torch.tensor([0.0]).to(device)\n","    if output_format == 'segmentation':\n","        weights = [128, 64, 32, 16, 8, 4, 2, 1, 128, 64, 32, 16, 8, 4, 2, 1]\n","    elif output_format == 'outlines':\n","        weights = [128, 64, 32, 16, 8, 4, 2, 1]\n","    for y_hat_i, y_i in zip(y_hat, y):\n","        loss_weights = torch.tensor(weights).to(device).repeat(y_i.shape[0], 1) # exponentially weight higher level headings more\n","        tt_loss += (torch.square(y_hat_i - y_i)).mul(loss_weights).mean()\n","    return tt_loss / len(y)"],"id":"6zQERFHtdJxK"},{"cell_type":"code","execution_count":null,"metadata":{"id":"lwySi2ltkv-1"},"outputs":[],"source":["def train(train_X, train_y, val_X, val_y, lr=0.002, batch_size=32, grad_clip=5.0, lr_decay=0.5,\n","          max_epoch=50, log_every=5, valid_niter=75, max_patience=4, max_num_trial=5, model_path='model.bin'):\n","    model = HeaderNet()\n","    model.train()\n","\n","    # # initialize model parameters\n","    # for p in model.parameters():\n","    #     p.data.uniform_(-0.1, 0.1)\n","\n","    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","    print('use device: %s' % device)\n","\n","    model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n","    print('{} parameters!'.format(sum([np.prod(p.size()) for p in model_parameters])))\n","\n","    model = model.to(device)\n","    optimizer = torch.optim.Adam(model.parameters(), lr)\n","\n","    num_trial = 0\n","    train_iter = patience = cum_loss = report_loss = cum_tgt_words = report_tgt_words = 0\n","    cum_examples = report_examples = epoch = valid_num = 0\n","    hist_valid_scores = []\n","    train_time = begin_time = time.time()\n","    print('begin Maximum Likelihood training')\n","\n","    train_losses = []\n","    val_losses = []\n","\n","    while True:\n","        epoch += 1\n","        batch_num = math.ceil(len(train_X) / batch_size)\n","        current_iter = 0\n","        for examples, labels in batch_iter(train_X, train_y, batch_size=batch_size, shuffle=True):\n","            model.train()\n","            current_iter += 1\n","            train_iter += 1\n","\n","            optimizer.zero_grad()\n","            batch_size = len(examples)\n","            train_loss = outline_loss(model(examples), get_segs(examples, labels))\n","            train_loss.backward()\n","\n","            # clip gradient\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n","\n","            optimizer.step()\n","\n","            report_loss += train_loss\n","            cum_loss += train_loss\n","            report_examples += batch_size\n","            cum_examples += batch_size\n","\n","            if train_iter % log_every == 0:\n","                print('epoch %d (%d / %d), iter %d, avg train loss %.2f, '\n","                      'cum examples %d, time elapsed %.2f sec' %\n","                      (epoch, current_iter, batch_num, train_iter,\n","                       report_loss / report_examples,\n","                       cum_examples,\n","                       time.time() - begin_time))\n","\n","                train_time = time.time()\n","                report_loss = report_examples = 0.\n","\n","            # perform validation\n","            if train_iter % valid_niter == 0:\n","                model.eval()\n","                with torch.no_grad():\n","                    print('epoch %d, iter %d, cum loss %.2f, cum examples %d' % (epoch, train_iter,\n","                            cum_loss / cum_examples,\n","                            cum_examples))\n","                    train_losses.append(cum_loss / cum_examples)\n","                    cum_loss = cum_examples = 0.\n","\n","                    print('begin validation ...')\n","\n","                    val_cum_loss = 0\n","                    val_cum_examples = 0\n","\n","                    count = 0\n","                    NUM_BATCHES = 16  # number of batches to validate over each time\n","                    for e, l in batch_iter(val_X, val_y, batch_size, shuffle=True):\n","                        if count >= NUM_BATCHES:\n","                            break\n","                        batch_size = len(e)\n","                        val_loss = outline_loss(model(e), get_segs(e, l))\n","                        val_cum_loss += val_loss\n","                        val_cum_examples += batch_size\n","                        count += 1\n","\n","                    val_losses.append(val_cum_loss / val_cum_examples)\n","                    valid_metric = -val_cum_loss / val_cum_examples # metric for evaluating whether model is improving on val data\n","\n","                    print('validation: iter %d, val loss %f' % (train_iter, val_loss))\n","\n","                    is_better = len(hist_valid_scores) == 0 or valid_metric > max(hist_valid_scores)\n","                    hist_valid_scores.append(valid_metric)\n","\n","                    if is_better:\n","                        patience = 0\n","                        print('epoch %d, iter %d: save currently the best model to [%s]' %\n","                                (epoch, train_iter, model_path))\n","                        model.save(model_path)\n","                        torch.save(optimizer.state_dict(), model_path + '.optim')\n","                        np.save('train.npy', np.array(train_losses))\n","                        np.save('val.npy', np.array(val_losses))\n","                    elif patience < max_patience:\n","                        patience += 1\n","                        print('hit patience %d' % patience)\n","\n","                        if patience == max_patience:\n","                            num_trial += 1\n","                            print('hit #%d trial' % num_trial)\n","                            if num_trial == max_num_trial:\n","                                print('early stop!')\n","                                exit(0)\n","\n","                            # decay lr, and restore from previously best checkpoint\n","                            lr = optimizer.param_groups[0]['lr'] * lr_decay\n","                            print('load previously best model and decay learning rate to %f' % lr)\n","\n","                            # load model\n","                            params = torch.load(model_path, map_location=lambda storage, loc: storage)\n","                            model.load_state_dict(params['state_dict'])\n","                            model = model.to(device)\n","                            train_losses = list(np.load('train.npy'))\n","                            val_losses = list(np.load('val.npy'))\n","\n","                            print('restore parameters of the optimizers')\n","                            optimizer.load_state_dict(torch.load(model_path + '.optim'))\n","\n","                            # set new lr\n","                            for param_group in optimizer.param_groups:\n","                                param_group['lr'] = lr\n","\n","                            # reset patience\n","                            patience = 0\n","\n","        if epoch == max_epoch:\n","            print('reached maximum number of epochs!')\n","            break"],"id":"lwySi2ltkv-1"},{"cell_type":"code","execution_count":null,"metadata":{"id":"1PVB2pZ9k2it"},"outputs":[],"source":["# train(train_X, train_y, val_X, val_y)"],"id":"1PVB2pZ9k2it"},{"cell_type":"code","execution_count":null,"metadata":{"id":"RkmXINGAoG2B"},"outputs":[],"source":["# print(data[0]['tree'].depth)\n","# print(data[0]['title'])\n","# get_outlines([data[0]['paragraphs']], [data[0]['tree']])"],"id":"RkmXINGAoG2B"},{"cell_type":"code","source":["device"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O35ektpjHJhM","executionInfo":{"status":"ok","timestamp":1651534242350,"user_tz":240,"elapsed":117,"user":{"displayName":"Jiatong Yu","userId":"17536159758394002445"}},"outputId":"8bd4195d-b032-4638-97e4-e400802995a0"},"id":"O35ektpjHJhM","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda', index=0)"]},"metadata":{},"execution_count":39}]},{"cell_type":"markdown","metadata":{"id":"HpclBBYffxaL"},"source":["##### Model 2: Greedy decoding + LCA loss"],"id":"HpclBBYffxaL"},{"cell_type":"code","execution_count":33,"metadata":{"executionInfo":{"elapsed":981,"status":"ok","timestamp":1651542133961,"user":{"displayName":"Michael Tang","userId":"18009171711496870090"},"user_tz":240},"id":"MgFl1lNDWKmv"},"outputs":[],"source":["class GreedyDecoder:\n","    def __init__(self, thresholds, similarity, encode):\n","        self.thresholds = thresholds\n","        self.similarity = similarity\n","        self.encode = encode\n","    \n","    # add level and parent info to a tree rooted at node\n","    def update_levels_parents(self, node, depth):\n","        # print(node.text)\n","        if len(node.children) == 0: # leaf paragraph node\n","            node.level = -1\n","            return\n","        node.level = depth\n","        for ch in node.children:\n","            ch.linkParent(node)\n","            self.update_levels_parents(ch, depth+1)\n","\n","    # decodes and returns tree rooted at node\n","    def encode_decode(self, paragraphs):\n","        embs = []\n","        for para in paragraphs:\n","            embs.append(self.encode(para))\n","        return self.decode(embs)\n","    \n","    # decodes and returns tree rooted at node, with text fields populated, e.g. for printing the tree\n","    def encode_decode_with_text(self, X, indexify=True):\n","        root = self.encode_decode(X)\n","        for idx, leaf in enumerate(preorder(root)):\n","            if indexify:\n","                leaf.text = idx\n","            else:\n","                leaf.text = X[idx]\n","        return root\n","    \n","    # here X and y are lists\n","    def batch_encode_decode_with_text(self, X):\n","        roots = []\n","        for X_i in X:\n","            roots.append(self.encode_decode_with_text(X_i))\n","        return roots\n","\n","    # bottom up decoding: for each depth, join paragraphs whose pairwise similarity reaches the threshold\n","    # and represent them collectively by the mean of their embeddings\n","    def decode(self, embs):\n","        if len(embs) == 0:\n","            return Node('', 0) # should never happen\n","        roots = []\n","        for i in range(len(embs)):\n","            roots.append(Node(i, -1))\n","        dim = embs[0].shape[0]\n","        for depth in range(MAX_DEPTH-1): # in last layer, everything must be joined together\n","            next_idxs = [[0]]\n","            for i in range(1, len(embs)):\n","                if self.similarity(embs[i-1], embs[i]) >= self.thresholds[depth]:\n","                    next_idxs[-1].append(i)\n","                else:\n","                    next_idxs.append([i])\n","\n","            # print(next_idxs)\n","            # for root in roots:\n","            #     print_tree(root)\n","\n","            # update roots and embs\n","            next_roots = []\n","            next_embs = []\n","            for comp in next_idxs:\n","                # don't add trivial (1 -> 1) edges\n","                if len(comp) == 1:\n","                    next_roots.append(roots[comp[0]])\n","                    next_embs.append(embs[comp[0]])\n","                    continue\n","\n","                next_roots.append(Node('', -2)) # meaningless params since we only need tree structure\n","                next_embs.append(torch.zeros(dim))\n","                for idx in comp:\n","                    next_roots[-1].insertChild(roots[idx])\n","                    next_embs[-1] += embs[idx]\n","                next_embs[-1] /= len(comp)\n","            roots = next_roots\n","            embs = next_embs\n","        \n","        # join everything together in the last layer\n","        root = Node('', 0)\n","        for node in roots:\n","            root.insertChild(node)\n","\n","        # update parents and levels, then return\n","        self.update_levels_parents(root, 0)\n","        return root"],"id":"MgFl1lNDWKmv"},{"cell_type":"markdown","source":["###### Evan's contrastive learning"],"metadata":{"id":"GErzYFrOwgVF"},"id":"GErzYFrOwgVF"},{"cell_type":"code","source":["# experimenting with para2vec :) \n","articles = [d['paragraphs'] for d in data]\n","paras = []\n","for doc in articles:\n","  for para in doc:\n","    paras.append(para)\n","samples = paras[:5]\n","input = tokenizer(samples[0], padding=True, truncation=True, return_tensors=\"pt\")"],"metadata":{"id":"MMCySoslxpac","executionInfo":{"status":"ok","timestamp":1651542135370,"user_tz":240,"elapsed":328,"user":{"displayName":"Michael Tang","userId":"18009171711496870090"}}},"id":"MMCySoslxpac","execution_count":34,"outputs":[]},{"cell_type":"code","source":["# playing with it :) \n","emb = para2vec_model(**input).pooler_output\n","emb=emb.reshape(-1,)\n","emb.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Joao3GzLmJDg","executionInfo":{"status":"ok","timestamp":1651542144400,"user_tz":240,"elapsed":1294,"user":{"displayName":"Michael Tang","userId":"18009171711496870090"}},"outputId":"f33daaaa-dfea-4b7c-f0d6-dcd6dc6342ab"},"id":"Joao3GzLmJDg","execution_count":35,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([768])"]},"metadata":{},"execution_count":35}]},{"cell_type":"code","source":["# dv_emb = torch.tensor(doc2vec_model.infer_vector(word_tokenize(samples[0])))\n","# dv_emb.shape"],"metadata":{"id":"rIbwWRyinw_U","executionInfo":{"status":"ok","timestamp":1651542144401,"user_tz":240,"elapsed":5,"user":{"displayName":"Michael Tang","userId":"18009171711496870090"}}},"id":"rIbwWRyinw_U","execution_count":36,"outputs":[]},{"cell_type":"code","execution_count":37,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1651542146685,"user":{"displayName":"Michael Tang","userId":"18009171711496870090"},"user_tz":240},"id":"rWo1Yua4hCjk"},"outputs":[],"source":["def doc2vec_sim(v1, v2):\n","    return torch.dot(v1, v2) / torch.norm(torch.sub(v1, v2))\n","\n","def doc2vec_enc(v):\n","    return torch.tensor(doc2vec_model.infer_vector(word_tokenize(v)))\n","# passing in a paragraph \n","def simcse_enc(v):\n","    para = re.sub('\\n', '', v)\n","    sents = re.split('[.]|[!]|[?]', para.strip())\n","    with io.capture_output() as captured:\n","        vecs = simcse_model.encode(sents, device=device, batch_size=len(sents))\n","    return torch.mean(vecs, dim=0)\n","\n","def para2vec_enc(v):\n","    intput = tokenizer(v, padding=True, truncation=True, return_tensors=\"pt\")\n","    emb = para2vec_model(**input).pooler_output\n","    return emb.reshape(-1)"],"id":"rWo1Yua4hCjk"},{"cell_type":"code","source":["enc1 = para2vec_enc('gibberish')"],"metadata":{"id":"t6BWvhxcxsGU","executionInfo":{"status":"ok","timestamp":1651545419552,"user_tz":240,"elapsed":957,"user":{"displayName":"Michael Tang","userId":"18009171711496870090"}}},"id":"t6BWvhxcxsGU","execution_count":42,"outputs":[]},{"cell_type":"code","source":["enc2 = para2vec_enc('a different breed of nothing')"],"metadata":{"id":"55P_mvCdxwWN","executionInfo":{"status":"ok","timestamp":1651545437502,"user_tz":240,"elapsed":388,"user":{"displayName":"Michael Tang","userId":"18009171711496870090"}}},"id":"55P_mvCdxwWN","execution_count":43,"outputs":[]},{"cell_type":"code","source":["enc3 = para2vec_enc('Lorde (born 1996) is a New Zealand singer-songwriter known for employing unconventional musical styles and thoughtful songwriting. She signed with Universal Music Group (UMG) in 2009 and collaborated with producer Joel Little in 2011 to record material. The pairs first collaborative effort, an extended play titled The Love Club, was released by UMG in 2013, and its international chart-topping single \"Royals\" helped Lorde rise to prominence. Her debut studio album Pure Heroine followed that year and achieved critical and commercial success. The following year, she curated the soundtrack for the 2014 film The Hunger Games: Mockingjay, Part 1 and recorded several tracks, including the single \"Yellow Flicker Beat\". Her second studio album Melodrama (2017) garnered widespread acclaim and reached number one on the US Billboard 200. Lordes music is primarily electropop and contains elements of subgenres such as dream pop and indie-electro. (This article is part of a featured topic: Overview of Lorde. Recently featured: 2021 World Snooker ChampionshipThe May PamphletEuropean nightjar ArchiveBy emailMore featured articles Did you know ... Recycling symbol Recycling symbol ... that the recycling symbol (pictured) depicts a Mbius strip? ... that Nick Schmaltzs older brother and younger sister would team up against him when they played basement hockey as children? ... that the former German chancellor Bernhard von Blow called Wilhelm IIs 1900 Hun speech the \"worst speech of that time and perhaps the most disgraceful speech that Wilhelm II [had] ever given\"? ... that if a disaster is avoided through planning and vigilance, people will paradoxically doubt that the preparation was necessary? ... that Port Vale F.C. is the only club to have beaten all 91 other clubs in the top four divisions of the current English football league system in a competitive league fixture?')"],"metadata":{"id":"c4FQN8ftx3gS","executionInfo":{"status":"ok","timestamp":1651545565833,"user_tz":240,"elapsed":313,"user":{"displayName":"Michael Tang","userId":"18009171711496870090"}}},"id":"c4FQN8ftx3gS","execution_count":50,"outputs":[]},{"cell_type":"code","source":["enc1==enc3"],"metadata":{"id":"YO5BhcMfyWtZ","executionInfo":{"status":"ok","timestamp":1651545570987,"user_tz":240,"elapsed":652,"user":{"displayName":"Michael Tang","userId":"18009171711496870090"}},"outputId":"e89a94a5-2058-42c1-f8c8-dcb1ffe5e19d","colab":{"base_uri":"https://localhost:8080/"}},"id":"YO5BhcMfyWtZ","execution_count":51,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n","        True, True, True, True, True, True, True, True, True, True, True, True,\n","        True, True, True, True, True, True, True, True, True, True, True, True,\n","        True, True, True, True, True, True, True, True, True, True, True, True,\n","        True, True, True, True, True, True, True, True, True, True, True, True,\n","        True, True, True, True, True, True, True, True, True, True, True, True,\n","        True, True, True, True, True, True, True, True, True, True, True, True,\n","        True, True, True, True, True, True, True, True, True, True, True, True,\n","        True, True, True, True, True, True, True, True, True, True, True, True,\n","        True, True, True, True, True, True, True, True, True, True, True, True,\n","        True, True, True, True, True, True, True, True, True, True, True, True,\n","        True, True, True, True, True, True, True, True, True, True, True, True,\n","        True, True, True, True, True, True, True, True, True, True, True, True,\n","        True, True, True, True, True, True, True, True, True, True, True, True,\n","        True, True, True, True, True, True, True, True, True, True, True, True,\n","        True, True, True, True, True, True, True, True, True, True, True, True,\n","        True, True, True, True, True, True, True, True, True, True, True, True,\n","        True, True, True, True, True, True, True, True, True, True, True, True,\n","        True, True, True, True, True, True, True, True, True, True, True, True,\n","        True, True, True, True, True, True, True, True, True, True, True, True,\n","        True, True, True, True, True, True, True, True, True, True, True, True,\n","        True, True, True, True, True, True, True, True, True, True, True, True,\n","        True, True, True, True, True, True, True, True, True, True, True, True,\n","        True, True, True, True, True, True, True, True, True, True, True, True,\n","        True, True, True, True, True, True, True, True, True, True, True, True,\n","        True, True, True, True, True, True, True, True, True, True, True, True,\n","        True, True, True, True, True, True, True, True, True, True, True, True,\n","        True, True, True, True, True, True, True, True, True, True, True, True,\n","        True, True, True, True, True, True, True, True, True, True, True, True,\n","        True, True, True, True, True, True, True, True, True, True, True, True,\n","        True, True, True, True, True, True, True, True, True, True, True, True,\n","        True, True, True, True, True, True, True, True, True, True, True, True,\n","        True, True, True, True, True, True, True, True, True, True, True, True,\n","        True, True, True, True, True, True, True, True, True, True, True, True,\n","        True, True, True, True, True, True, True, True, True, True, True, True,\n","        True, True, True, True, True, True, True, True, True, True, True, True,\n","        True, True, True, True, True, True, True, True, True, True, True, True,\n","        True, True, True, True, True, True, True, True, True, True, True, True,\n","        True, True, True, True, True, True, True, True, True, True, True, True,\n","        True, True, True, True, True, True, True, True, True, True, True, True,\n","        True, True, True, True, True, True, True, True, True, True, True, True,\n","        True, True, True, True, True, True, True, True, True, True, True, True,\n","        True, True, True, True, True, True, True, True, True, True, True, True,\n","        True, True, True, True, True, True, True, True, True, True, True, True,\n","        True, True, True, True, True, True, True, True, True, True, True, True,\n","        True, True, True, True, True, True, True, True, True, True, True, True,\n","        True, True, True, True, True, True, True, True, True, True, True, True,\n","        True, True, True, True, True, True, True, True, True, True, True, True,\n","        True, True, True, True, True, True, True, True, True, True, True, True,\n","        True, True, True, True, True, True, True, True, True, True, True, True,\n","        True, True, True, True, True, True, True, True, True, True, True, True,\n","        True, True, True, True, True, True, True, True, True, True, True, True,\n","        True, True, True, True, True, True, True, True, True, True, True, True,\n","        True, True, True, True, True, True, True, True, True, True, True, True,\n","        True, True, True, True, True, True, True, True, True, True, True, True,\n","        True, True, True, True, True, True, True, True, True, True, True, True,\n","        True, True, True, True, True, True, True, True, True, True, True, True,\n","        True, True, True, True, True, True, True, True, True, True, True, True,\n","        True, True, True, True, True, True, True, True, True, True, True, True,\n","        True, True, True, True, True, True, True, True, True, True, True, True,\n","        True, True, True, True, True, True, True, True, True, True, True, True,\n","        True, True, True, True, True, True, True, True, True, True, True, True,\n","        True, True, True, True, True, True, True, True, True, True, True, True,\n","        True, True, True, True, True, True, True, True, True, True, True, True])"]},"metadata":{},"execution_count":51}]},{"cell_type":"code","execution_count":38,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1651542148415,"user":{"displayName":"Michael Tang","userId":"18009171711496870090"},"user_tz":240},"id":"njWi7UimNnjJ"},"outputs":[],"source":["def evaluate_thresholds(X, y, thresholds, similarity=doc2vec_sim, encode=doc2vec_enc):\n","    greedy = GreedyDecoder(thresholds, similarity=similarity, encode=encode)\n","    y_hat = greedy.batch_encode_decode_with_text(X)\n","    y = [indexified_tree(y_i.root) for y_i in y]\n","    lens = [len(x) for x in X]\n","    return batch_lca_loss(y, y_hat, lens)\n","    # print('Actual:')\n","    # print_tree(y_hat[0])\n","    # print('Predicted:')\n","    # print_tree(y[0])\n","\n","def random_search(X, y, n_tries=50, min_num=[0]*8, max_num=[3]*8, similarity=doc2vec_sim, encode=doc2vec_enc):\n","    results = []\n","    for i in range(n_tries):\n","        print('try #' + str(i))\n","        thresholds = []\n","        for j in range(8):\n","            thresholds.append(random.uniform(min_num[j],max_num[j]))\n","        # print('thresholds:', thresholds)\n","        loss = evaluate_thresholds(X, y, thresholds, similarity=similarity, encode=encode)\n","        results.append({'loss':loss, 'thresholds':thresholds})\n","    \n","    # print sorted version by each index\n","    print('printing sorted by component...')\n","    for i in range(8):\n","        print('component', i)\n","        results_sorted = sorted(results, key=lambda x: x['thresholds'][i])\n","        for result in results_sorted:\n","            thresh =  [\"{0:0.5f}\".format(i) for i in result['thresholds']]\n","            print(result['loss'], '\\t', thresh)\n","        print()"],"id":"njWi7UimNnjJ"},{"cell_type":"code","source":["len(test_X)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6Sj0O2EmiAga","executionInfo":{"status":"ok","timestamp":1651542148740,"user_tz":240,"elapsed":3,"user":{"displayName":"Michael Tang","userId":"18009171711496870090"}},"outputId":"f6869e6d-1f15-4db0-e313-76bdb7517de2"},"id":"6Sj0O2EmiAga","execution_count":39,"outputs":[{"output_type":"execute_result","data":{"text/plain":["59"]},"metadata":{},"execution_count":39}]},{"cell_type":"code","execution_count":40,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EP42xnCssegT","outputId":"4191b945-cd41-4610-94ee-23d87e81ef5e","executionInfo":{"status":"ok","timestamp":1651543365207,"user_tz":240,"elapsed":1214946,"user":{"displayName":"Michael Tang","userId":"18009171711496870090"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["try #0\n","try #1\n","try #2\n","try #3\n","try #4\n","try #5\n","try #6\n","try #7\n","try #8\n","try #9\n","printing sorted by component...\n","component 0\n","1.395529285246924 \t ['0.39196', '0.43186', '2.54653', '1.02416', '1.00634', '2.95633', '1.62898', '0.68879']\n","1.395529285246924 \t ['0.46769', '2.07137', '0.35494', '2.05736', '0.12311', '1.76948', '2.91686', '1.48071']\n","1.395529285246924 \t ['0.90139', '1.56830', '0.09893', '2.40093', '0.23881', '1.24156', '1.95090', '0.57560']\n","1.395529285246924 \t ['1.02985', '1.56111', '0.38611', '2.07373', '1.30532', '1.19653', '1.98111', '0.28068']\n","1.395529285246924 \t ['1.05639', '2.28808', '2.68538', '2.45834', '1.50595', '2.03027', '1.17378', '2.13637']\n","1.395529285246924 \t ['1.15645', '0.02187', '1.63367', '1.66779', '2.35306', '2.59658', '1.40402', '2.66346']\n","1.395529285246924 \t ['1.71314', '1.59469', '0.85871', '1.01017', '0.16554', '0.64434', '0.77211', '0.35688']\n","1.395529285246924 \t ['2.06872', '0.96558', '0.04218', '0.83699', '1.86735', '2.19658', '0.22194', '1.19364']\n","1.395529285246924 \t ['2.16779', '0.76102', '2.78307', '0.71293', '1.25748', '0.65141', '0.44111', '1.23556']\n","1.395529285246924 \t ['2.92012', '0.69049', '1.70937', '2.79231', '0.28276', '1.08702', '1.24105', '2.15934']\n","\n","component 1\n","1.395529285246924 \t ['1.15645', '0.02187', '1.63367', '1.66779', '2.35306', '2.59658', '1.40402', '2.66346']\n","1.395529285246924 \t ['0.39196', '0.43186', '2.54653', '1.02416', '1.00634', '2.95633', '1.62898', '0.68879']\n","1.395529285246924 \t ['2.92012', '0.69049', '1.70937', '2.79231', '0.28276', '1.08702', '1.24105', '2.15934']\n","1.395529285246924 \t ['2.16779', '0.76102', '2.78307', '0.71293', '1.25748', '0.65141', '0.44111', '1.23556']\n","1.395529285246924 \t ['2.06872', '0.96558', '0.04218', '0.83699', '1.86735', '2.19658', '0.22194', '1.19364']\n","1.395529285246924 \t ['1.02985', '1.56111', '0.38611', '2.07373', '1.30532', '1.19653', '1.98111', '0.28068']\n","1.395529285246924 \t ['0.90139', '1.56830', '0.09893', '2.40093', '0.23881', '1.24156', '1.95090', '0.57560']\n","1.395529285246924 \t ['1.71314', '1.59469', '0.85871', '1.01017', '0.16554', '0.64434', '0.77211', '0.35688']\n","1.395529285246924 \t ['0.46769', '2.07137', '0.35494', '2.05736', '0.12311', '1.76948', '2.91686', '1.48071']\n","1.395529285246924 \t ['1.05639', '2.28808', '2.68538', '2.45834', '1.50595', '2.03027', '1.17378', '2.13637']\n","\n","component 2\n","1.395529285246924 \t ['2.06872', '0.96558', '0.04218', '0.83699', '1.86735', '2.19658', '0.22194', '1.19364']\n","1.395529285246924 \t ['0.90139', '1.56830', '0.09893', '2.40093', '0.23881', '1.24156', '1.95090', '0.57560']\n","1.395529285246924 \t ['0.46769', '2.07137', '0.35494', '2.05736', '0.12311', '1.76948', '2.91686', '1.48071']\n","1.395529285246924 \t ['1.02985', '1.56111', '0.38611', '2.07373', '1.30532', '1.19653', '1.98111', '0.28068']\n","1.395529285246924 \t ['1.71314', '1.59469', '0.85871', '1.01017', '0.16554', '0.64434', '0.77211', '0.35688']\n","1.395529285246924 \t ['1.15645', '0.02187', '1.63367', '1.66779', '2.35306', '2.59658', '1.40402', '2.66346']\n","1.395529285246924 \t ['2.92012', '0.69049', '1.70937', '2.79231', '0.28276', '1.08702', '1.24105', '2.15934']\n","1.395529285246924 \t ['0.39196', '0.43186', '2.54653', '1.02416', '1.00634', '2.95633', '1.62898', '0.68879']\n","1.395529285246924 \t ['1.05639', '2.28808', '2.68538', '2.45834', '1.50595', '2.03027', '1.17378', '2.13637']\n","1.395529285246924 \t ['2.16779', '0.76102', '2.78307', '0.71293', '1.25748', '0.65141', '0.44111', '1.23556']\n","\n","component 3\n","1.395529285246924 \t ['2.16779', '0.76102', '2.78307', '0.71293', '1.25748', '0.65141', '0.44111', '1.23556']\n","1.395529285246924 \t ['2.06872', '0.96558', '0.04218', '0.83699', '1.86735', '2.19658', '0.22194', '1.19364']\n","1.395529285246924 \t ['1.71314', '1.59469', '0.85871', '1.01017', '0.16554', '0.64434', '0.77211', '0.35688']\n","1.395529285246924 \t ['0.39196', '0.43186', '2.54653', '1.02416', '1.00634', '2.95633', '1.62898', '0.68879']\n","1.395529285246924 \t ['1.15645', '0.02187', '1.63367', '1.66779', '2.35306', '2.59658', '1.40402', '2.66346']\n","1.395529285246924 \t ['0.46769', '2.07137', '0.35494', '2.05736', '0.12311', '1.76948', '2.91686', '1.48071']\n","1.395529285246924 \t ['1.02985', '1.56111', '0.38611', '2.07373', '1.30532', '1.19653', '1.98111', '0.28068']\n","1.395529285246924 \t ['0.90139', '1.56830', '0.09893', '2.40093', '0.23881', '1.24156', '1.95090', '0.57560']\n","1.395529285246924 \t ['1.05639', '2.28808', '2.68538', '2.45834', '1.50595', '2.03027', '1.17378', '2.13637']\n","1.395529285246924 \t ['2.92012', '0.69049', '1.70937', '2.79231', '0.28276', '1.08702', '1.24105', '2.15934']\n","\n","component 4\n","1.395529285246924 \t ['0.46769', '2.07137', '0.35494', '2.05736', '0.12311', '1.76948', '2.91686', '1.48071']\n","1.395529285246924 \t ['1.71314', '1.59469', '0.85871', '1.01017', '0.16554', '0.64434', '0.77211', '0.35688']\n","1.395529285246924 \t ['0.90139', '1.56830', '0.09893', '2.40093', '0.23881', '1.24156', '1.95090', '0.57560']\n","1.395529285246924 \t ['2.92012', '0.69049', '1.70937', '2.79231', '0.28276', '1.08702', '1.24105', '2.15934']\n","1.395529285246924 \t ['0.39196', '0.43186', '2.54653', '1.02416', '1.00634', '2.95633', '1.62898', '0.68879']\n","1.395529285246924 \t ['2.16779', '0.76102', '2.78307', '0.71293', '1.25748', '0.65141', '0.44111', '1.23556']\n","1.395529285246924 \t ['1.02985', '1.56111', '0.38611', '2.07373', '1.30532', '1.19653', '1.98111', '0.28068']\n","1.395529285246924 \t ['1.05639', '2.28808', '2.68538', '2.45834', '1.50595', '2.03027', '1.17378', '2.13637']\n","1.395529285246924 \t ['2.06872', '0.96558', '0.04218', '0.83699', '1.86735', '2.19658', '0.22194', '1.19364']\n","1.395529285246924 \t ['1.15645', '0.02187', '1.63367', '1.66779', '2.35306', '2.59658', '1.40402', '2.66346']\n","\n","component 5\n","1.395529285246924 \t ['1.71314', '1.59469', '0.85871', '1.01017', '0.16554', '0.64434', '0.77211', '0.35688']\n","1.395529285246924 \t ['2.16779', '0.76102', '2.78307', '0.71293', '1.25748', '0.65141', '0.44111', '1.23556']\n","1.395529285246924 \t ['2.92012', '0.69049', '1.70937', '2.79231', '0.28276', '1.08702', '1.24105', '2.15934']\n","1.395529285246924 \t ['1.02985', '1.56111', '0.38611', '2.07373', '1.30532', '1.19653', '1.98111', '0.28068']\n","1.395529285246924 \t ['0.90139', '1.56830', '0.09893', '2.40093', '0.23881', '1.24156', '1.95090', '0.57560']\n","1.395529285246924 \t ['0.46769', '2.07137', '0.35494', '2.05736', '0.12311', '1.76948', '2.91686', '1.48071']\n","1.395529285246924 \t ['1.05639', '2.28808', '2.68538', '2.45834', '1.50595', '2.03027', '1.17378', '2.13637']\n","1.395529285246924 \t ['2.06872', '0.96558', '0.04218', '0.83699', '1.86735', '2.19658', '0.22194', '1.19364']\n","1.395529285246924 \t ['1.15645', '0.02187', '1.63367', '1.66779', '2.35306', '2.59658', '1.40402', '2.66346']\n","1.395529285246924 \t ['0.39196', '0.43186', '2.54653', '1.02416', '1.00634', '2.95633', '1.62898', '0.68879']\n","\n","component 6\n","1.395529285246924 \t ['2.06872', '0.96558', '0.04218', '0.83699', '1.86735', '2.19658', '0.22194', '1.19364']\n","1.395529285246924 \t ['2.16779', '0.76102', '2.78307', '0.71293', '1.25748', '0.65141', '0.44111', '1.23556']\n","1.395529285246924 \t ['1.71314', '1.59469', '0.85871', '1.01017', '0.16554', '0.64434', '0.77211', '0.35688']\n","1.395529285246924 \t ['1.05639', '2.28808', '2.68538', '2.45834', '1.50595', '2.03027', '1.17378', '2.13637']\n","1.395529285246924 \t ['2.92012', '0.69049', '1.70937', '2.79231', '0.28276', '1.08702', '1.24105', '2.15934']\n","1.395529285246924 \t ['1.15645', '0.02187', '1.63367', '1.66779', '2.35306', '2.59658', '1.40402', '2.66346']\n","1.395529285246924 \t ['0.39196', '0.43186', '2.54653', '1.02416', '1.00634', '2.95633', '1.62898', '0.68879']\n","1.395529285246924 \t ['0.90139', '1.56830', '0.09893', '2.40093', '0.23881', '1.24156', '1.95090', '0.57560']\n","1.395529285246924 \t ['1.02985', '1.56111', '0.38611', '2.07373', '1.30532', '1.19653', '1.98111', '0.28068']\n","1.395529285246924 \t ['0.46769', '2.07137', '0.35494', '2.05736', '0.12311', '1.76948', '2.91686', '1.48071']\n","\n","component 7\n","1.395529285246924 \t ['1.02985', '1.56111', '0.38611', '2.07373', '1.30532', '1.19653', '1.98111', '0.28068']\n","1.395529285246924 \t ['1.71314', '1.59469', '0.85871', '1.01017', '0.16554', '0.64434', '0.77211', '0.35688']\n","1.395529285246924 \t ['0.90139', '1.56830', '0.09893', '2.40093', '0.23881', '1.24156', '1.95090', '0.57560']\n","1.395529285246924 \t ['0.39196', '0.43186', '2.54653', '1.02416', '1.00634', '2.95633', '1.62898', '0.68879']\n","1.395529285246924 \t ['2.06872', '0.96558', '0.04218', '0.83699', '1.86735', '2.19658', '0.22194', '1.19364']\n","1.395529285246924 \t ['2.16779', '0.76102', '2.78307', '0.71293', '1.25748', '0.65141', '0.44111', '1.23556']\n","1.395529285246924 \t ['0.46769', '2.07137', '0.35494', '2.05736', '0.12311', '1.76948', '2.91686', '1.48071']\n","1.395529285246924 \t ['1.05639', '2.28808', '2.68538', '2.45834', '1.50595', '2.03027', '1.17378', '2.13637']\n","1.395529285246924 \t ['2.92012', '0.69049', '1.70937', '2.79231', '0.28276', '1.08702', '1.24105', '2.15934']\n","1.395529285246924 \t ['1.15645', '0.02187', '1.63367', '1.66779', '2.35306', '2.59658', '1.40402', '2.66346']\n","\n"]}],"source":["random_search(train_X[20:40], train_y[20:40], n_tries=10, encode=para2vec_enc, min_num=[0,0,0,0,0,0,0,0], max_num=[3,3,3,3,3,3,3,3])"],"id":"EP42xnCssegT"},{"cell_type":"code","execution_count":null,"metadata":{"id":"5cUSNy2ATU6C"},"outputs":[],"source":["def add_tree_breaks(node, breaks, cur_breaks, depths, depth):\n","    if node.level == -1: # this is paragraph node, not a heading\n","        depths.append(depth)\n","        breaks.append(cur_breaks)\n","        return\n","    for child in node.children:\n","        if child == node.children[0]: # first entry\n","            cur_breaks += 1\n","            depth = node.level\n","        if child == node.children[-1]:\n","            cur_breaks = 0\n","            depth = 0\n","        add_tree_breaks(child, breaks, cur_breaks, depths, depth)\n","        if child == node.children[0]:\n","            cur_breaks = 0\n","            depth = 0\n","        if child == node.children[-1]:\n","            cur_breaks = 0\n","            depth = 0\n","    cur_breaks = 0\n","\n","def get_breaks(paras, trees):\n","    max_paras = max([len(x) for x in paras])\n","    breaks = []\n","    depths = []\n","    for tree in trees:\n","        breaks_i = []\n","        depths_i = []\n","        add_tree_breaks(tree.root, breaks_i, 0, depths_i, 0)\n","        breaks_i = torch.tensor(breaks_i)\n","        depths_i[0] += 1\n","        depths_i = torch.tensor(depths_i)\n","        breaks.append(breaks_i)\n","        depths.append(depths_i)\n","    return breaks, depths\n","\n","def convert_dataset(dataset, window_size):\n","  breaks, depths = get_breaks([d['paragraphs'] for d in dataset], [d['tree'] for d in dataset])\n","  X = []\n","  D = []\n","  y = []\n","  for i in tqdm(range(len(dataset))):  # for article\n","      # print('{}'.format(100 * i / len(dataset)))\n","      article = dataset[i]['paragraphs']\n","      for p in range(1, len(article)):  # for para in article (excluding first, since its clearly always a break)\n","          depth = depths[i][p]\n","          isBreak = breaks[i][p]\n","          for b in range(1, depth + 1):  # for depths <= depth of para\n","              context = []\n","              for j in range(p - window_size, p + window_size + 1):  # for para in context\n","                  if j < 0 or j >= len(article):\n","                      context.append(None)\n","                  else:\n","                      context.append(article[j])\n","              X.append(context)\n","              D.append(np.array([b]))\n","              val = 1 if b > depth - isBreak else 0  # 1 if target para is first after break at depth b\n","              y.append(np.array([val]))\n","          if depth == 0:\n","              for b in range(1, 8):  # MAX_DEPTH is 8\n","                  context = []\n","                  for j in range(p - window_size, p + window_size + 1):  # for para in context\n","                      if j < 0 or j >= len(article):\n","                        context.append(None)\n","                      else:\n","                        context.append(article[j])\n","                  X.append(context)\n","                  D.append(np.array([0]))\n","                  y.append(np.array([0]))\n","  return X, torch.tensor(np.stack(D, axis=0)), torch.tensor(np.stack(y, axis=0))\n","\n","def get_split_data(data, window_size, train_ratio=0.8, val_ratio=0.0001, test_ratio=0.0001):\n","    random.shuffle(data)\n","    n_data = len(data)\n","    train_idx = int(train_ratio * n_data)\n","    val_idx = int((train_ratio + val_ratio) * n_data)\n","    test_idx = int((train_ratio + val_ratio + test_ratio) * n_data)\n","    print('getting training data')\n","    train_data = convert_dataset(data[:train_idx], window_size=window_size)\n","    print('getting val data')\n","    val_data = convert_dataset(data[train_idx:val_idx], window_size=window_size)\n","    print('getting test data')\n","    test_data = convert_dataset(data[val_idx:test_idx], window_size=window_size)\n","    return train_data, val_data, test_data\n","\n","# iterate over batches of data and labels\n","def batch_iter(data, batch_size, shuffle=False):\n","    X, D, y = data\n","    batch_num = math.ceil(len(X) / batch_size)\n","    index_array = list(range(len(X)))\n","\n","    if shuffle:\n","        np.random.shuffle(index_array)\n","\n","    for i in range(batch_num):\n","        indices = index_array[i * batch_size: (i + 1) * batch_size]\n","        batch_data_X = [X[idx] for idx in indices]\n","        batch_data_D = D[indices]\n","        batch_data_y = y[indices]\n","\n","        yield batch_data_X, batch_data_D, batch_data_y\n","\n","# MODULES FOR EMBEDDING 3 DIFFERENT WAYS: DOC2VEC, PROJECTED SIMCSE, PROJECTED FASTTEXT\n","# each one takes a batch of lists of paragraphs, outputs a batch of concatenated paragraph embeddings\n","# forward pass input: batch (len B) of list of paragraphs (len 2 * window_size + 1), each para variable length\n","# forward pass output: tensor of size B x ((2 * window_size + 1) * emb_dim)\n","class Doc2VecEmbedding(nn.Module):\n","  def __init__(self, window_size, emb_dim): \n","    super().__init__()\n","    self.emb_dim = emb_dim\n","    self.doc2vec = Doc2Vec.load(\"models/{}_d2v.model\".format(emb_dim))\n","\n","  def forward(self, x):\n","    with torch.no_grad():\n","      batch = []\n","      for b in x:\n","        paras = []\n","        for p in b:\n","          paras.append(self.doc2vec.infer_vector(word_tokenize(p.lower())) if p is not None else np.zeros(shape=(self.emb_dim)))\n","        batch.append(np.concatenate(paras, axis=0))\n","      return torch.tensor(np.stack(batch, axis=0)).to(device)\n","\n","class SimCSEEmbedding(nn.Module):\n","  def __init__(self, window_size, emb_dim, dropout): \n","    super().__init__()\n","    self.window_size = window_size\n","    self.emb_dim = emb_dim\n","    self.simcse = SimCSE(\"princeton-nlp/sup-simcse-bert-base-uncased\")\n","    self.SIMCSE_DIM = 768 # dim of simcse sentence embeddings\n","    self.lstm = nn.LSTM(input_size=self.SIMCSE_DIM,\n","                        hidden_size=int(emb_dim / 4),\n","                        num_layers=1,\n","                        bidirectional=True,\n","                        batch_first=True, \n","                        dropout=0.0).to(device)\n","\n","  def forward(self, x):\n","    B = len(x)  # batch size\n","    batch = []\n","    with torch.no_grad():\n","      for b in x:\n","        for p in b:\n","          if p is not None:\n","            p = re.sub('\\n', '', p)\n","            sents = re.split('[.]|[!]|[?]', p.strip())\n","            with io.capture_output() as captured:\n","                sents_emb = self.simcse.encode(sents, device=device, batch_size=len(sents), max_length=128)  # a tensor of len(sents) x SIMCSE_DIM\n","            batch.append(sents_emb)\n","          else:\n","            batch.append(torch.zeros((1, self.SIMCSE_DIM), device=device))\n","      batch_emb = torch.nn.utils.rnn.pad_sequence(batch, batch_first=True).to(device)\n","      packed_in = torch.nn.utils.rnn.pack_padded_sequence(batch_emb, torch.tensor([a.shape[0] for a in batch_emb]), batch_first=True, enforce_sorted=False)\n","\n","    _, (hidden, cell) = self.lstm(packed_in.to(device))\n","    para_embs = torch.cat((hidden[0], cell[0], hidden[1], cell[1]), dim=1)        \n","      \n","    return para_embs.reshape(B, (2 * self.window_size + 1) * self.emb_dim)\n","\n","\n","#### HERE\n","class FastTextEmbedding(nn.Module):\n","  def __init__(self, window_size, emb_dim, dropout): \n","    super().__init__()\n","    self.window_size = window_size\n","    self.emb_dim = emb_dim\n","    self.fasttext = FastText.load_fasttext_format('models/fast-text-300.bin')\n","    self.FASTTEXT_DIM = 300 # dim of fasttext word embeddings\n","    self.lstm = nn.LSTM(input_size=self.FASTTEXT_DIM,\n","                        hidden_size=int(emb_dim / 4),\n","                        num_layers=1,\n","                        bidirectional=True,\n","                        batch_first=True, \n","                        dropout=0.0).to(device)\n","\n","  def forward(self, x):\n","    B = len(x)  # batch size\n","    batch = []\n","    with torch.no_grad():\n","      for b in x:\n","        for p in b:\n","          if p is not None:\n","            p = re.sub('\\n', '', p)\n","            words = re.sub(\"[^\\s\\w]\", \"\", p.strip()).split(' ')\n","            words = list(filter(None, words))\n","            words_emb = torch.stack([torch.tensor(self.fasttext[word]) for word in words]).to(device)  # a tensor of len(words) x FASTTEXT_DIM\n","            batch.append(words_emb)\n","          else:\n","            batch.append(torch.zeros((1, self.FASTTEXT_DIM), device=device))\n","      batch_emb = torch.nn.utils.rnn.pad_sequence(batch, batch_first=True).to(device)\n","      packed_in = torch.nn.utils.rnn.pack_padded_sequence(batch_emb, torch.tensor([a.shape[0] for a in batch_emb]), batch_first=True, enforce_sorted=False)\n","\n","    _, (hidden, cell) = self.lstm(packed_in.to(device))\n","    para_embs = torch.cat((hidden[0], cell[0], hidden[1], cell[1]), dim=1)        \n","      \n","    return para_embs.reshape(B, (2 * self.window_size + 1) * self.emb_dim)\n","\n","class MLP(nn.Module):\n","  def __init__(self, layer_dims, window_size, emb_dim, emb_method, dropout=0.1):\n","        super().__init__()\n","        self.window_size = window_size\n","        self.emb_dim = emb_dim # dimension of each paragraph embedding\n","        if emb_method == 'doc2vec':\n","          self.emb = Doc2VecEmbedding(window_size, emb_dim)\n","        elif emb_method == 'simcse':\n","          self.emb = SimCSEEmbedding(window_size, emb_dim, dropout=dropout)\n","        elif emb_method == 'fasttext':\n","          self.emb = FastTextEmbedding(window_size, emb_dim, dropout=dropout)\n","        else:\n","          raise NotImplementedError()\n","\n","        in_feats = (2 * window_size + 1) * self.emb_dim\n","        self.layers = []\n","        for dim in layer_dims:\n","          self.layers.append(nn.Linear(in_features=in_feats + 1, out_features=dim))\n","          in_feats = dim\n","        self.layers.append(nn.Linear(in_features=in_feats + 1, out_features=1))\n","        self.layers = nn.ModuleList(self.layers)\n","        self.dropout = nn.Dropout(dropout)\n","\n","  def forward(self, x, d):\n","      '''\n","      x is a batch (list) of windows (list) of paragraphs, which are strings\n","      d is the depths for the whole batch (tensor)\n","      '''\n","      B = len(x)\n","      x = self.emb(x).float()  # x is a (B x (2W + 1) x E) tensor\n","      x = x.reshape(B, (2 * self.window_size + 1) * self.emb_dim)\n","      for layer in self.layers[:-1]:\n","        x = F.relu(layer(torch.cat((x, d), dim=1)))\n","      x = self.dropout(x)\n","      x = torch.sigmoid(self.layers[-1](torch.cat((x, d), dim=1)))\n","      return x\n","\n","  def recursive_outline(self, subarticle, node):\n","    if len(subarticle) == 1:\n","        new = Node(subarticle[0], -1)\n","        new.linkParent(node)\n","        node.insertChild(new)\n","        return\n","    outs = [False]\n","    for p in range(1, len(subarticle)):\n","        context = []\n","        for j in range(p - self.window_size, p + self.window_size + 1):\n","            if j < 0 or j >= len(subarticle):\n","                context.append(None)\n","            else:\n","                context.append(subarticle[j])\n","        X = [context]\n","        D = torch.tensor([node.level + 1]).to(device).unsqueeze(dim=0)\n","        out = self.forward(X, D).squeeze()\n","        outs.append(out.cpu().item() > 0.77)\n","    prev = 0\n","    flag = True\n","    for o in range(len(outs)):\n","        if outs[o]:\n","            new = Node('', node.level + 1)\n","            new.linkParent(node)\n","            node.insertChild(new)\n","            self.recursive_outline(subarticle[prev:o], new)\n","            prev = o\n","            flag = False\n","    if flag:\n","      for p in range(len(subarticle)):\n","        new = Node(subarticle[p], -1)\n","        new.linkParent(node)\n","        node.insertChild(new)\n","\n","        # else:\n","        #     new = Node(subarticle[o], -1)\n","        #     new.linkParent(node)\n","        #     node.insertChild(new)\n","    else:\n","       new = Node('', node.level + 1)\n","       new.linkParent(node)\n","       node.insertChild(new)\n","       self.recursive_outline(subarticle[prev:], new)\n","    return\n","\n","  def outline(self, article, wordy=False):\n","      self.eval()\n","      root = Node('root', 1)\n","      self.recursive_outline(article, root)\n","      new = Node(article[0], -1)\n","      curr = root\n","      while len(curr.children) > 0 and curr.level != -1:\n","        curr = curr.children[0]\n","      new.linkParent(curr.parent)\n","      curr.parent.insertChild(new)\n","      if len(article) > 1:\n","          new = Node(article[len(article)-1], -1)\n","          curr = root\n","          while len(curr.children) > 0 and curr.level != -1:\n","            curr = curr.children[-1]\n","          new.linkParent(curr.parent)\n","          curr.parent.insertChild(new)\n","\n","      def printNode(curNode):\n","          print(curNode.level, '       ', curNode.text)\n","          if curNode.level == -1:\n","              return\n","\n","          for child in curNode.children:\n","              printNode(child)\n","          return\n","\n","      if wordy:\n","        printNode(root)\n","      return root\n","\n","  def save(self, path: str):\n","      \"\"\" Save the model to a file.\n","      @param path (str): path to the model\n","      \"\"\"\n","\n","      params = {\n","          # 'args': dict(hid_dim=self.hid_dim, n_layers=self.n_layers, num_heads=self.num_heads,\n","          #       num_enc_layers=self.num_enc_layers, num_dec_layers=self.num_dec_layers, ff_dim=self.ff_dim, dropout=self.dropout),\n","          'state_dict': self.state_dict()\n","      }\n","\n","      torch.save(params, path)"],"id":"5cUSNy2ATU6C"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"ltaPAg3hSu8f"},"outputs":[],"source":["# getting losses from recursive MLP\n","\n","# MODEL SPECS\n","WINDOW_SIZE = 3  # number of neighbors to consider in each direction\n","EMB_DIM = 256  # dim each paragraph becomes, via magic :)\n","EMB_METHOD = 'simcse'  # one of 'doc2vec', 'simcse', 'fasttext'\n","MLP_ARCHITECTURE = [1024, 256, 64]  # sizes of hidden layers in MLP\n","\n","model = MLP(layer_dims=MLP_ARCHITECTURE, window_size=WINDOW_SIZE, emb_dim=EMB_DIM, emb_method=EMB_METHOD)\n","\n","# EVAL STUFF\n","params = torch.load('checkpoints/mlp_{}.bin'.format(EMB_METHOD), map_location=lambda storage, loc: storage)\n","model.load_state_dict(params['state_dict'])\n","model = model.to(device)\n","\n","# article = data[20]['paragraphs']\n","X = test_X\n","y = test_y\n","roots = [indexified_tree(model.outline(a)) for a in X]"],"id":"ltaPAg3hSu8f"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"uf-FPRECUs9n","outputId":"f2b872a3-1a6b-464a-b46e-94bf2f342b09"},"outputs":[{"data":{"text/plain":["4.588360066568925"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["golds = [indexified_tree(a.root) for a in y]\n","lens = [len(a) for a in X]\n","batch_lca_loss(roots, golds, lens)"],"id":"uf-FPRECUs9n"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"wFtlYPvBTwxO"},"outputs":[],"source":["y_hat = roots\n","y = golds"],"id":"wFtlYPvBTwxO"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":35777,"status":"ok","timestamp":1651481996840,"user":{"displayName":"Michael Tang","userId":"18009171711496870090"},"user_tz":240},"id":"kPYJXOv5boxM","outputId":"613a3185-6399-4278-fd0a-e379777ea6d5"},"outputs":[{"name":"stdout","output_type":"stream","text":["2.7905591455488716\n"]}],"source":["# thresholds = [1.769, 1.261, 0.007, 2.289, 1.793, 0.590, 3.620, 3.969] # 128 thresholds\n","thresholds = [1.769, 1.261, 0.007, 2.289, 1.793, 0.590, 3.620, 3.969] # 256 thresholds\n","# thresholds = [1.668, 0.429, 0.810, 2.581, 1.015, 1.205, 1.755, 0.440] # simcse thresholds\n","X = test_X\n","y = test_y\n","\n","greedy = GreedyDecoder(thresholds, similarity=doc2vec_sim, encode=doc2vec_enc)\n","# greedy = GreedyDecoder(thresholds, similarity=doc2vec_sim, encode=simcse_enc)\n","y_hat = greedy.batch_encode_decode_with_text(X)\n","y = [indexified_tree(y_i.root) for y_i in y]\n","lens = [len(x) for x in X]\n","print(batch_lca_loss(y, y_hat, lens))\n","\n","# for i in range(10):\n","#     print(i)\n","#     print('Predicted:')\n","#     print_tree(y_hat[i])\n","#     print('Actual:')\n","#     print_tree(y[i])"],"id":"kPYJXOv5boxM"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18086,"status":"ok","timestamp":1651481821275,"user":{"displayName":"Michael Tang","userId":"18009171711496870090"},"user_tz":240},"id":"KqzMYmxLwtpD","outputId":"94b90283-ee3b-4645-df84-74adf726117b"},"outputs":[{"name":"stdout","output_type":"stream","text":["1.8826851358941468\n"]}],"source":["print(batch_lca_loss(y, y_hat, lens))"],"id":"KqzMYmxLwtpD"},{"cell_type":"code","execution_count":null,"metadata":{"id":"p5Uh2CSQ3bJ2"},"outputs":[],"source":["model_losses = []\n","for len_i, y_i, y_hat_i in zip(lens, y, y_hat):\n","    model_losses.append(lca_loss(y_i, y_hat_i, len_i))"],"id":"p5Uh2CSQ3bJ2"},{"cell_type":"code","execution_count":null,"metadata":{"id":"SWVHchil3ovC"},"outputs":[],"source":["import csv\n","from google.colab import files\n","\n","with open(\"mlp_loss.csv\", \"w\") as f:\n","    writer = csv.writer(f)\n","    writer.writerow(model_losses)\n","files.download('mlp_loss.csv')"],"id":"SWVHchil3ovC"},{"cell_type":"markdown","metadata":{"id":"1j2UAawngVgV"},"source":["Here we simulate the degenerate output from the end-to-end model to get LCA loss of that model"],"id":"1j2UAawngVgV"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1825,"status":"ok","timestamp":1651477975112,"user":{"displayName":"Michael Tang","userId":"18009171711496870090"},"user_tz":240},"id":"z4uUx5hYl0sP","outputId":"553ef939-bbbe-4dd9-fc71-8e9557d442f9"},"outputs":[{"name":"stdout","output_type":"stream","text":["4.731525515214907\n"]}],"source":["bad_node = Node(-1,0)\n","for i in range(100):\n","    bad_node.insertChild(Node(i,-1))\n","\n","# thresholds = [1.769, 1.261, 0.007, 2.289, 1.793, 0.590, 3.620, 3.969]\n","X = val_X\n","y = val_y\n","\n","greedy = GreedyDecoder(thresholds, similarity=doc2vec_sim, encode=doc2vec_enc)\n","y_hat = [bad_node]*len(y)\n","y = [indexified_tree(y_i.root) for y_i in y]\n","lens = [len(x) for x in X]\n","print(batch_lca_loss(y, y_hat, lens))"],"id":"z4uUx5hYl0sP"},{"cell_type":"markdown","metadata":{"id":"rmhSuAZMgbYO"},"source":["We can hard-code to extract specific results here:"],"id":"rmhSuAZMgbYO"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":208,"status":"ok","timestamp":1650535212167,"user":{"displayName":"Michael Tang","userId":"18009171711496870090"},"user_tz":240},"id":"dPaeL9nneQ-c","outputId":"564ef17e-e5a6-41a0-b251-9289dd010c28"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'M @-@ 331 ( Michigan highway )'"]},"execution_count":347,"metadata":{},"output_type":"execute_result"}],"source":["val_y[10].root.text"],"id":"dPaeL9nneQ-c"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":130,"status":"ok","timestamp":1650535513374,"user":{"displayName":"Michael Tang","userId":"18009171711496870090"},"user_tz":240},"id":"fjFZlDETfXTI","outputId":"a2b2d9a6-634c-4998-9fee-9e25c2e8c820"},"outputs":[{"name":"stdout","output_type":"stream","text":["  [heading]\n","    [heading]\n","      [heading]\n","        [heading]\n","           Dan Dugan ( born March 20 , 1943 ) is a...\n","           In his youth , Dugan was fascinated by ...\n","           Dugan first recorded sounds in the late...\n","         Daniel W. Dugan was born in Los Angeles...\n","        [heading]\n","           Dugan changed from lighting design to s...\n","           Dugan designed sound for three regional...\n","           When Margrit Mondavi founded the Mondav...\n","      [heading]\n","         Dugan occasionally delivered speeches a...\n","        [heading]\n","           While designing sound for the musical H...\n","           Though the algorithm was good , the ref...\n","           \" I was messing around with logarithmic...\n","         Dugan licensed this more practical syst...\n","        [heading]\n","           In the late 1980s , Dugan developed a g...\n","           Dugan 's original 1974 patent expired i...\n","        [heading]\n","           In September 2006 , Dugan produced the ...\n","           In February 2011 , Dugan demonstrated a...\n","         Dugan made his first sound effects reco...\n","         Dugan and his wife Sharon Perry , the N...\n","         \" There are three potential values in s...\n","         In 2006 , Dugan assisted a group of res...\n","         In 1998 an organization he co @-@ found...\n","         As co @-@ founder and Secretary of PLAN...\n"]}],"source":["print_snippet_tree(textified_tree(y_hat[7], val_X[7]))"],"id":"fjFZlDETfXTI"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":171,"status":"ok","timestamp":1650535518477,"user":{"displayName":"Michael Tang","userId":"18009171711496870090"},"user_tz":240},"id":"CHzt_3Bug96V","outputId":"197bd189-78ba-4aab-98d7-937f28cd0ad5"},"outputs":[{"name":"stdout","output_type":"stream","text":["  [heading]\n","     Dan Dugan ( born March 20 , 1943 ) is a...\n","     In his youth , Dugan was fascinated by ...\n","     Dugan first recorded sounds in the late...\n","    [heading]\n","       Daniel W. Dugan was born in Los Angeles...\n","    [heading]\n","       Dugan changed from lighting design to s...\n","       Dugan designed sound for three regional...\n","       When Margrit Mondavi founded the Mondav...\n","       Dugan occasionally delivered speeches a...\n","    [heading]\n","       While designing sound for the musical H...\n","       Though the algorithm was good , the ref...\n","       \" I was messing around with logarithmic...\n","       Dugan licensed this more practical syst...\n","       In the late 1980s , Dugan developed a g...\n","       Dugan 's original 1974 patent expired i...\n","       In September 2006 , Dugan produced the ...\n","       In February 2011 , Dugan demonstrated a...\n","    [heading]\n","       Dugan made his first sound effects reco...\n","       Dugan and his wife Sharon Perry , the N...\n","       \" There are three potential values in s...\n","       In 2006 , Dugan assisted a group of res...\n","    [heading]\n","       In 1998 an organization he co @-@ found...\n","       As co @-@ founder and Secretary of PLAN...\n"]}],"source":["print_snippet_tree(textified_tree(y[7], val_X[7]))"],"id":"CHzt_3Bug96V"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9540,"status":"ok","timestamp":1650530936386,"user":{"displayName":"Michael Tang","userId":"18009171711496870090"},"user_tz":240},"id":"B2KLLiYbO87K","outputId":"2462738e-e520-47c3-d29e-7a7f5587aad3"},"outputs":[{"name":"stdout","output_type":"stream","text":["45.15277996900072\n","Actual:\n","[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], [16, 17, 18, 19], 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]\n","Predicted:\n","[0, 1, [2, 3, 4, 5], [6, 7], [8, 9], [10, 11, 12], [13, 14], [15], [16, [17, 18, 19, 20, 21], [22, 23], [24, 25, 26, 27, 28, 29, 30, 31]]]\n"]}],"source":["evaluate_thresholds(train_X[:100], train_y[:100], [1.3,3,2,2,8,8,8,8])"],"id":"B2KLLiYbO87K"},{"cell_type":"markdown","metadata":{"id":"wStwFp0dgjKJ"},"source":["And here we look at the doc2vec distributions"],"id":"wStwFp0dgjKJ"},{"cell_type":"code","execution_count":null,"metadata":{"id":"WZNawe-4QdLy"},"outputs":[],"source":["# investigate distribution of Doc2Vec distributions\n","def plot_similarity_distribution(similarity=doc2vec_sim, encode=simcse_enc):\n","    sims = []\n","    n = len(train_X[0])\n","    for x in range(n):\n","        for y in range(x+1,n):\n","            # print(doc2vec_enc(train_X[0][x]), doc2vec_enc(train_X[0][y]))\n","            sims.append(similarity(encode(train_X[0][x]), encode(train_X[0][y])))\n","    plt.hist(sims)\n","    plt.xlabel('Similarity')\n","    plt.ylabel('Count')\n","    plt.show()"],"id":"WZNawe-4QdLy"},{"cell_type":"code","execution_count":null,"metadata":{"id":"f7wFwTidrCH4"},"outputs":[],"source":["plot_similarity_distribution()"],"id":"f7wFwTidrCH4"},{"cell_type":"markdown","metadata":{"id":"yc5HGcLzgoFv"},"source":["##### Model 3: Recursive split MLP -- note this code may have a decoding bug"],"id":"yc5HGcLzgoFv"},{"cell_type":"code","execution_count":null,"metadata":{"id":"Lr1puvn7hOmI"},"outputs":[],"source":["WINDOW_SIZE = 2  # WINDOW_SIZE paragraphs on each side of target para are considered\n","DOC2VEC_DIM = 128"],"id":"Lr1puvn7hOmI"},{"cell_type":"code","execution_count":null,"metadata":{"id":"cvpuvYJnhci0"},"outputs":[],"source":["def add_tree_breaks(node, breaks, cur_breaks, depths, depth):\n","    if node.level == -1: # this is paragraph node, not a heading\n","        depths.append(depth)\n","        breaks.append(cur_breaks)\n","        return\n","    for child in node.children:\n","        if child == node.children[0]: # first entry\n","            cur_breaks += 1\n","            depth = node.level\n","        if child == node.children[-1]:\n","            cur_breaks = 0\n","            depth = 0\n","        add_tree_breaks(child, breaks, cur_breaks, depths, depth)\n","        if child == node.children[0]:\n","            cur_breaks = 0\n","            depth = 0\n","        if child == node.children[-1]:\n","            cur_breaks = 0\n","            depth = 0\n","    cur_breaks = 0\n","\n","def get_breaks(paras, trees):\n","    max_paras = max([len(x) for x in paras])\n","    breaks = []\n","    depths = []\n","    for tree in trees:\n","        breaks_i = []\n","        depths_i = []\n","        add_tree_breaks(tree.root, breaks_i, 0, depths_i, 0)\n","        breaks_i = torch.tensor(breaks_i)\n","        depths_i[0] += 1\n","        depths_i = torch.tensor(depths_i)\n","        breaks.append(breaks_i)\n","        depths.append(depths_i)\n","    return breaks, depths\n","\n","def convert_dataset(dataset):\n","  breaks, depths = get_breaks([d['paragraphs'] for d in dataset], [d['tree'] for d in dataset])\n","  X = []\n","  D = []\n","  y = []\n","  for i in range(len(dataset)):  # for article\n","      print('{}'.format(100 * i / len(dataset)))\n","      article = dataset[i]['paragraphs']\n","      for p in range(1, len(article)):  # for para in article (excluding first, since its clearly always a break)\n","          depth = depths[i][p]\n","          isBreak = breaks[i][p]\n","          for b in range(1, depth + 1):  # for depths <= depth of para\n","              context = []\n","              for j in range(p - WINDOW_SIZE, p + WINDOW_SIZE + 1):  # for para in context\n","                  if j < 0 or j >= len(article):\n","                      context.append(np.zeros([DOC2VEC_DIM]))\n","                  else:\n","                      context.append(doc2vec_model.infer_vector(word_tokenize(article[j].lower())))\n","              X.append(np.concatenate(context, axis=0))\n","              D.append(np.array([b]))\n","              val = 1 if b > depth - isBreak else 0  # 1 if target para is first after break at depth b\n","              y.append(np.array([val]))\n","          if depth == 0:\n","              for b in range(1, MAX_DEPTH):\n","                  context = []\n","                  for j in range(p - WINDOW_SIZE, p + WINDOW_SIZE + 1):  # for para in context\n","                      if j < 0 or j >= len(article):\n","                          context.append(np.zeros([DOC2VEC_DIM]))\n","                      else:\n","                          context.append(doc2vec_model.infer_vector(word_tokenize(article[j].lower())))\n","                  X.append(np.concatenate(context, axis=0))\n","                  D.append(np.array([0]))\n","                  y.append(np.array([0]))\n","  return np.stack(X, axis=0), np.stack(D, axis=0), np.stack(y, axis=0)"],"id":"cvpuvYJnhci0"},{"cell_type":"code","execution_count":null,"metadata":{"id":"1Ce1cQI_hjKm"},"outputs":[],"source":["def get_split_data(data, train_ratio=0.0008, val_ratio=0.0001, test_ratio=0.0001):\n","    random.shuffle(data)\n","    n_data = len(data)\n","    train_idx = int(train_ratio * n_data)\n","    val_idx = int((train_ratio + val_ratio) * n_data)\n","    test_idx = int((train_ratio + val_ratio + test_ratio) * n_data)\n","    train_data = convert_dataset(data[:train_idx])\n","    val_data = convert_dataset(data[train_idx:val_idx])\n","    test_data = convert_dataset(data[val_idx:test_idx])\n","    return train_data, val_data, test_data"],"id":"1Ce1cQI_hjKm"},{"cell_type":"code","execution_count":null,"metadata":{"id":"uQIdIQvAheHi"},"outputs":[],"source":["train_data, val_data, test_data = get_split_data(data)"],"id":"uQIdIQvAheHi"},{"cell_type":"code","execution_count":null,"metadata":{"id":"D5oIYe9shkio"},"outputs":[],"source":["# iterate over batches of data and labels\n","# articles in batch are sorted by descending num paragraphs for efficient packing/LSTM inference\n","def batch_iter(data, batch_size, shuffle=False):\n","    batch_num = math.ceil(len(dataX) / batch_size)\n","    index_array = list(range(len(dataX)))\n","\n","    if shuffle:\n","        np.random.shuffle(index_array)\n","\n","    for i in range(batch_num):\n","        indices = index_array[i * batch_size: (i + 1) * batch_size]\n","        batch_data_X = torch.tensor(dataX[indices])\n","        batch_data_D = torch.tensor(dataD[indices])\n","        batch_data_y = torch.tensor(datay[indices])\n","\n","        yield batch_data_X, batch_data_D, batch_data_y"],"id":"D5oIYe9shkio"},{"cell_type":"code","execution_count":null,"metadata":{"id":"K2ayqAuIhqjt"},"outputs":[],"source":["class MLP(nn.Module):\n","  def __init__(self, layer_dims, dropout=0.1):\n","        super().__init__()\n","        in_feats = (2 * WINDOW_SIZE + 1) * DOC2VEC_DIM\n","        self.layers = []\n","        for dim in layer_dims:\n","          self.layers.append(nn.Linear(in_features=in_feats + 1, out_features=dim))\n","          in_feats = dim\n","        self.layers.append(nn.Linear(in_features=in_feats + 1, out_features=1))\n","        self.layers = nn.ModuleList(self.layers)\n","        self.dropout = nn.Dropout(dropout)\n","\n","  def forward(self, x, d):\n","      for layer in self.layers[:-1]:\n","        x = F.relu(layer(torch.cat((x, d), dim=1)))\n","      x = self.dropout(x)\n","      x = torch.sigmoid(self.layers[-1](torch.cat((x, d), dim=1)))\n","      return x\n","\n","  def recursive_outline(self, subarticle, node):\n","    if len(subarticle) == 1:\n","        new = Node(subarticle[0], -1)\n","        new.linkParent(node)\n","        node.insertChild(new)\n","        return\n","    outs = [False]\n","    for p in range(1, len(subarticle)):\n","        context = []\n","        for j in range(p - WINDOW_SIZE, p + WINDOW_SIZE + 1):\n","            if j < 0 or j >= len(subarticle):\n","                context.append(np.zeros([DOC2VEC_DIM]))\n","            else:\n","                context.append(doc2vec_model.infer_vector(word_tokenize(subarticle[j].lower())))\n","        X = torch.tensor(np.concatenate(context, axis=0), device=device, dtype=torch.float32).unsqueeze(dim=0)\n","        D = torch.tensor([node.level + 1]).to(device).unsqueeze(dim=0)\n","        out = self.forward(X, D).squeeze()\n","        outs.append(out.cpu().item() > 0.77)\n","    prev = 0\n","    flag = True\n","    for o in range(len(outs)):\n","        if outs[o]:\n","            new = Node('', node.level + 1)\n","            new.linkParent(node)\n","            node.insertChild(new)\n","            self.recursive_outline(subarticle[prev:o], new)\n","            prev = o\n","            flag = False\n","    if flag:\n","      for p in range(len(subarticle)):\n","        new = Node(subarticle[p], -1)\n","        new.linkParent(node)\n","        node.insertChild(new)\n","\n","        # else:\n","        #     new = Node(subarticle[o], -1)\n","        #     new.linkParent(node)\n","        #     node.insertChild(new)\n","    else:\n","       new = Node('', node.level + 1)\n","       new.linkParent(node)\n","       node.insertChild(new)\n","       self.recursive_outline(subarticle[prev:], new)\n","    return\n","\n","  def outline(self, article, wordy=False):\n","      self.eval()\n","      root = Node('root', 1)\n","      self.recursive_outline(article, root)\n","      new = Node(article[0], -1)\n","      curr = root\n","      while len(curr.children) > 0 and curr.level != -1:\n","        curr = curr.children[0]\n","      new.linkParent(curr.parent)\n","      curr.parent.insertChild(new)\n","      if len(article) > 1:\n","          new = Node(article[len(article)-1], -1)\n","          curr = root\n","          while len(curr.children) > 0 and curr.level != -1:\n","            curr = curr.children[-1]\n","          new.linkParent(curr.parent)\n","          curr.parent.insertChild(new)\n","\n","      def printNode(curNode):\n","          print(curNode.level, '       ', curNode.text)\n","          if curNode.level == -1:\n","              return\n","\n","          for child in curNode.children:\n","              printNode(child)\n","          return\n","\n","      if wordy:\n","        printNode(root)\n","      return root\n","\n","  @staticmethod\n","  def load(model_path: str):\n","      \"\"\" Load the model from a file.\n","      @param model_path (str): path to model\n","      \"\"\"\n","      params = torch.load(model_path, map_location=lambda storage, loc: storage)\n","      # args = params['args']\n","      model = MLP(layer_dims=[1024, 256, 64])\n","      model.load_state_dict(params['state_dict'])\n","\n","      return model\n","\n","  def save(self, path: str):\n","      \"\"\" Save the model to a file.\n","      @param path (str): path to the model\n","      \"\"\"\n","\n","      params = {\n","          # 'args': dict(hid_dim=self.hid_dim, n_layers=self.n_layers, num_heads=self.num_heads,\n","          #       num_enc_layers=self.num_enc_layers, num_dec_layers=self.num_dec_layers, ff_dim=self.ff_dim, dropout=self.dropout),\n","          'state_dict': self.state_dict()\n","      }\n","\n","      torch.save(params, path)"],"id":"K2ayqAuIhqjt"},{"cell_type":"code","execution_count":null,"metadata":{"id":"kFhfncT-hruW"},"outputs":[],"source":["def train(train_data, val_data, lr=0.002, batch_size=1, grad_clip=5.0, lr_decay=0.5,\n","          max_epoch=50, log_every=5, valid_niter=25, max_patience=4, max_num_trial=5, model_path='model.bin'):\n","    model = MLP(layer_dims=[1024, 256, 64])\n","    model.train()\n","\n","    # # initialize model parameters\n","    # for p in model.parameters():\n","    #     p.data.uniform_(-0.1, 0.1)\n","\n","    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","    print('use device: %s' % device)\n","\n","    model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n","    print('{} parameters!'.format(sum([np.prod(p.size()) for p in model_parameters])))\n","\n","    model = model.to(device)\n","    optimizer = torch.optim.Adam(model.parameters(), lr)\n","\n","    num_trial = 0\n","    train_iter = patience = cum_loss = report_loss = cum_tgt_words = report_tgt_words = 0\n","    cum_examples = report_examples = epoch = valid_num = 0\n","    hist_valid_scores = []\n","    train_time = begin_time = time.time()\n","    print('begin Maximum Likelihood training')\n","\n","    train_losses = []\n","    val_losses = []\n","    loss_fn = nn.BCELoss(reduction='sum')\n","\n","    train_X = np.load('train_X.npy', allow_pickle=True)\n","    train_D = np.load('train_D.npy', allow_pickle=True)\n","    train_y = np.load('train_y.npy', allow_pickle=True)\n","    \n","    # train_X, train_D, train_y = train_data\n","    val_X, val_D, val_y = val_data\n","\n","    while True:\n","        epoch += 1\n","        batch_num = math.ceil(len(train_data) / batch_size)\n","        current_iter = 0\n","        for batch in batch_iter(train_data, batch_size=batch_size, shuffle=True):\n","            X, D, y = convert_dataset(batch)\n","            X = X.to(device)\n","            D = D.to(device)\n","            y = y.to(dtype=torch.float32, device=device)\n","\n","            model.train()\n","            current_iter += 1\n","            train_iter += 1\n","\n","            optimizer.zero_grad()\n","            batch_size = len(X)\n","            out = model(X, D)\n","            train_loss = loss_fn(out, y)\n","            train_loss.backward()\n","\n","            # clip gradient\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n","\n","            optimizer.step()\n","\n","            report_loss += train_loss.item()\n","            cum_loss += train_loss.item()\n","            report_examples += batch_size\n","            cum_examples += batch_size\n","\n","            if train_iter % log_every == 0:\n","                print('epoch %d (%d / %d), iter %d, avg train loss %f, '\n","                      'cum examples %d, time elapsed %.2f sec' %\n","                      (epoch, current_iter, batch_num, train_iter,\n","                       report_loss / report_examples,\n","                       cum_examples,\n","                       time.time() - begin_time))\n","\n","                train_time = time.time()\n","                report_loss = report_examples = 0.\n","\n","            # perform validation\n","            if train_iter % valid_niter == 0:\n","                model.eval()\n","                with torch.no_grad():\n","                    print('epoch %d, iter %d, cum loss %f, cum examples %d' % (epoch, train_iter,\n","                            cum_loss / cum_examples,\n","                            cum_examples))\n","                    train_losses.append(cum_loss / cum_examples)\n","                    cum_loss = cum_examples = 0.\n","\n","                    print('begin validation ...')\n","\n","                    val_cum_loss = 0.\n","                    val_cum_examples = 0\n","\n","                    count = 0\n","                    NUM_BATCHES = 8  # number of batches to validate over each time\n","                    for batch in batch_iter(val_data, batch_size, shuffle=True):\n","                        if count >= NUM_BATCHES:\n","                            break\n","                        X, D, y = convert_dataset(batch)\n","                        X = X.to(device)\n","                        D = D.to(device)\n","                        y = y.to(dtype=torch.float32, device=device)\n","\n","                        batch_size = len(X)\n","                        out = model(X, D)\n","                        val_loss = loss_fn(out, y)\n","                        val_cum_loss += val_loss.item()\n","                        val_cum_examples += batch_size\n","                        count += 1\n","\n","                    val_losses.append(val_cum_loss / val_cum_examples)\n","                    valid_metric = -val_cum_loss / val_cum_examples # metric for evaluating whether model is improving on val data\n","\n","                    print('validation: iter %d, val loss %f' % (train_iter, val_cum_loss / val_cum_examples))\n","\n","                    is_better = len(hist_valid_scores) == 0 or valid_metric > max(hist_valid_scores)\n","                    hist_valid_scores.append(valid_metric)\n","\n","                    if is_better:\n","                        patience = 0\n","                        print('epoch %d, iter %d: save currently the best model to [%s]' %\n","                                (epoch, train_iter, model_path))\n","                        model.save(model_path)\n","                        torch.save(optimizer.state_dict(), model_path + '.optim')\n","                        np.save('train.npy', np.array(train_losses))\n","                        np.save('val.npy', np.array(val_losses))\n","                    elif patience < max_patience:\n","                        patience += 1\n","                        print('hit patience %d' % patience)\n","\n","                        if patience == max_patience:\n","                            num_trial += 1\n","                            print('hit #%d trial' % num_trial)\n","                            if num_trial == max_num_trial:\n","                                print('early stop!')\n","                                exit(0)\n","\n","                            # decay lr, and restore from previously best checkpoint\n","                            lr = optimizer.param_groups[0]['lr'] * lr_decay\n","                            print('load previously best model and decay learning rate to %f' % lr)\n","\n","                            # load model\n","                            params = torch.load(model_path, map_location=lambda storage, loc: storage)\n","                            model.load_state_dict(params['state_dict'])\n","                            model = model.to(device)\n","                            train_losses = list(np.load('train.npy', allow_pickle=True))\n","                            val_losses = list(np.load('val.npy', allow_pickle=True))\n","\n","                            print('restore parameters of the optimizers')\n","                            optimizer.load_state_dict(torch.load(model_path + '.optim'))\n","\n","                            # set new lr\n","                            for param_group in optimizer.param_groups:\n","                                param_group['lr'] = lr\n","\n","                            # reset patience\n","                            patience = 0\n","\n","        if epoch == max_epoch:\n","            print('reached maximum number of epochs!')\n","            break"],"id":"kFhfncT-hruW"},{"cell_type":"code","execution_count":null,"metadata":{"id":"3hy4A11xhwzd"},"outputs":[],"source":["# train(train_data, val_data)\n","model = MLP(layer_dims=[1024, 256, 64])\n","params = torch.load('model.bin', map_location=lambda storage, loc: storage)\n","model.load_state_dict(params['state_dict'])\n","model = model.to(device)\n","\n","# article = data[20]['paragraphs']\n","sample = data[200:350]\n","roots = [indexified_tree(model.outline(a['paragraphs'])) for a in sample]\n","golds = [indexified_tree(a['tree'].root) for a in sample]\n","lens = [len(a['paragraphs']) for a in sample]\n","batch_lca_loss(roots, golds, lens)"],"id":"3hy4A11xhwzd"},{"cell_type":"markdown","metadata":{"id":"MUazcP3Jkote"},"source":["##### Human performance"],"id":"MUazcP3Jkote"},{"cell_type":"code","execution_count":null,"metadata":{"id":"2fxv4m2Lkrjp"},"outputs":[],"source":["def get_paragraphs_trees(data):\n","    paras = [d['paragraphs'] for d in data]\n","    trees = [d['tree'] for d in data]\n","    return paras, trees\n","\n","def get_split_data(data, train_ratio=0.05, val_ratio=0.001, test_ratio=0.01):\n","    random.shuffle(data)\n","    n_data = len(data)\n","    train_idx = int(train_ratio*n_data)\n","    val_idx = int((train_ratio + val_ratio)*n_data)\n","    test_idx = int((train_ratio + val_ratio + test_ratio)*n_data)\n","    train_X, train_y = get_paragraphs_trees(data[:train_idx])\n","    val_X, val_y = get_paragraphs_trees(data[train_idx:val_idx])\n","    test_X, test_y = get_paragraphs_trees(data[val_idx:test_idx])\n","    return train_X, train_y, val_X, val_y, test_X, test_y\n","\n","train_X, train_y, val_X, val_y, test_X, test_y = get_split_data(data)"],"id":"2fxv4m2Lkrjp"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":248,"status":"ok","timestamp":1651486912093,"user":{"displayName":"Michael Tang","userId":"18009171711496870090"},"user_tz":240},"id":"dNKULJxXlK9b","outputId":"396b80a9-b6af-4b2f-8caa-33e19e68a86d"},"outputs":[{"data":{"text/plain":["[' Olympic record ; NR = National record \\n',\n"," \" Three Egyptian swimmers , one female and two males , qualified for the Olympics . American @-@ born Farida Osman , the woman , was the youngest member of the 2012 national delegation and qualified for the Games based on having made the Olympic Standard Time in the 50 @-@ metre freestyle . In the lead up to the Olympics , she won seven gold medals at the 2011 Pan Arab Games , as well as the 50 @-@ metre butterfly title at that year 's All @-@ Africa Games and World Junior Championships . In London she was sixth in her heat in the 50 @-@ metre freestyle and was eliminated from the tournament , ranking 42nd overall . On the men 's side , <unk> Younis qualified for the 50 @-@ metre freestyle by meeting the Olympic Selection Time of 22 @.@ 88 at an international meet in Eindhoven , Netherlands , with a result of 22 @.@ 85 . Prior to the Olympics he had won a bronze medal in the event at the 2011 Pan Arab Games . In London he was third in his heat and 34th overall , failing to advance to the semifinals . Mazen <unk> , a Saudi @-@ born Egyptian training and studying at Southern Illinois University Carbondale , made the team at an Olympic qualifier in Setbal , Portugal in June 2012 , the second of his two opportunities ( his first having been the 2011 World Championships ) . At the Games , in the marathon 10 kilometre , he was 24th out of 25 competitors . \\n\",\n"," ' Men \\n',\n"," ' Women \\n',\n"," ' Key : Q \\n']"]},"execution_count":56,"metadata":{},"output_type":"execute_result"}],"source":["test_X[3]"],"id":"dNKULJxXlK9b"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"executionInfo":{"elapsed":724,"status":"ok","timestamp":1651371101651,"user":{"displayName":"Michael Tang","userId":"07765995532208265709"},"user_tz":240},"id":"CBy8GjAPYONc","outputId":"e8f915e8-ea13-43c7-8a33-4ac1ba48f51b"},"outputs":[{"data":{"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/javascript":["download(\"download_aa6d52cd-2048-480f-bf77-49516b228010\", \"test_X.csv\", 5407643)"],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{},"output_type":"display_data"}],"source":["import csv\n","from google.colab import files\n","\n","with open(\"test_X.csv\", \"w\") as f:\n","    writer = csv.writer(f)\n","    writer.writerows(test_X)\n","files.download('test_X.csv')"],"id":"CBy8GjAPYONc"},{"cell_type":"code","execution_count":null,"metadata":{"id":"iAKemnWSv4z6"},"outputs":[],"source":["# from tree_utils import *"],"id":"iAKemnWSv4z6"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"executionInfo":{"elapsed":260,"status":"ok","timestamp":1651444885243,"user":{"displayName":"Michael Tang","userId":"07765995532208265709"},"user_tz":240},"id":"sq53vVSNEwdg","outputId":"ff689c8c-f5e9-4c71-edb7-344a3b455c3a"},"outputs":[{"data":{"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/javascript":["download(\"download_47c187a0-d839-4f00-9958-9e93cf703847\", \"test_y.pickle\", 6014345)"],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{},"output_type":"display_data"}],"source":["import pickle\n","from google.colab import files\n","\n","with open(\"test_y.pickle\", \"wb\") as f:\n","    pickle.dump(test_y, f)\n","files.download('test_y.pickle')"],"id":"sq53vVSNEwdg"}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["0ba86871","e49e36c2","FSKtCnA_BdMD"],"name":"[someone run this]Greedy+contrastive embedding.ipynb","provenance":[{"file_id":"1MGfIx5mwYvwfJTGO2w5ySa7D-1S4-FOd","timestamp":1650353357566}],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"},"widgets":{"application/vnd.jupyter.widget-state+json":{"c1f56010394a4de6ad0d6606512961bf":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fc7773686c0345859691d1012b46300e","IPY_MODEL_7bedbae29b864d54b2d2aa71305a7e85","IPY_MODEL_7c52fa240a584372b3d2fe641e0a26c4"],"layout":"IPY_MODEL_c2b75d0567d1405c825e9420e296761e"}},"fc7773686c0345859691d1012b46300e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2f4732f535394dc583c773bccd4064fc","placeholder":"","style":"IPY_MODEL_e9c2052cd89e4fab9654e820a4191085","value":"Downloading builder script: "}},"7bedbae29b864d54b2d2aa71305a7e85":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_df7b6449ca5d4af692726a9fa0627b87","max":2032,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a6b53968bcc041c5bf1b41ce39fdea91","value":2032}},"7c52fa240a584372b3d2fe641e0a26c4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bc410147e6274f2285d5c170ac3705a1","placeholder":"","style":"IPY_MODEL_bf53104080a547e08a3d42d7a096c621","value":" 8.48k/? [00:00&lt;00:00, 290kB/s]"}},"c2b75d0567d1405c825e9420e296761e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2f4732f535394dc583c773bccd4064fc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e9c2052cd89e4fab9654e820a4191085":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"df7b6449ca5d4af692726a9fa0627b87":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a6b53968bcc041c5bf1b41ce39fdea91":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"bc410147e6274f2285d5c170ac3705a1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bf53104080a547e08a3d42d7a096c621":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b4c2854f450944c3ac4e1e2e2c8c52fc":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_52e6283ba63141108d1733cc934c566a","IPY_MODEL_fff4a0959cd8400394b5769b3a51a497","IPY_MODEL_63275b065caa4e578eff42ce9c24bfd2"],"layout":"IPY_MODEL_bb77ca82ab46482b91b2dac242805d8d"}},"52e6283ba63141108d1733cc934c566a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f553d8ae53d74da182d51a65735934d6","placeholder":"","style":"IPY_MODEL_70f6c6080bcd4772b038bf6b4d66ce71","value":"Downloading metadata: "}},"fff4a0959cd8400394b5769b3a51a497":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3f83868c2b7f45d1b30c359c1b460357","max":1250,"min":0,"orientation":"horizontal","style":"IPY_MODEL_585a521fab47472eb13254ff05e67be3","value":1250}},"63275b065caa4e578eff42ce9c24bfd2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_00cc2499804b4b81b860087d81c90e2d","placeholder":"","style":"IPY_MODEL_2944ead59ab549468af252bbaa220070","value":" 6.84k/? [00:00&lt;00:00, 255kB/s]"}},"bb77ca82ab46482b91b2dac242805d8d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f553d8ae53d74da182d51a65735934d6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"70f6c6080bcd4772b038bf6b4d66ce71":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3f83868c2b7f45d1b30c359c1b460357":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"585a521fab47472eb13254ff05e67be3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"00cc2499804b4b81b860087d81c90e2d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2944ead59ab549468af252bbaa220070":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f7ab22541968478f85b6dc94f766ad95":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1f217b335fca4de189857609b4e3ea54","IPY_MODEL_fa2e652439334b8cb7af8c1248bbc764","IPY_MODEL_a1bd627fcf204cd48f93d23bfe22ab5d"],"layout":"IPY_MODEL_dbbcd5d2fdcf4d408c47e3f016937489"}},"1f217b335fca4de189857609b4e3ea54":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0e0f715c376c47f08603643d1d834480","placeholder":"","style":"IPY_MODEL_d3f6d546db024d88a003e61fbf7d3609","value":"Downloading data: 100%"}},"fa2e652439334b8cb7af8c1248bbc764":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ef9a194b5b624c6b9cdf91829ebd7acb","max":190229076,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d24770109d8b4b49870195597844a69c","value":190229076}},"a1bd627fcf204cd48f93d23bfe22ab5d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_94cf94ba79d54a8f8e6f4a6f974a2e13","placeholder":"","style":"IPY_MODEL_e8082992754c4d2c93199077669ab085","value":" 190M/190M [00:19&lt;00:00, 11.5MB/s]"}},"dbbcd5d2fdcf4d408c47e3f016937489":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0e0f715c376c47f08603643d1d834480":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d3f6d546db024d88a003e61fbf7d3609":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ef9a194b5b624c6b9cdf91829ebd7acb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d24770109d8b4b49870195597844a69c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"94cf94ba79d54a8f8e6f4a6f974a2e13":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e8082992754c4d2c93199077669ab085":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0d247affe5fc401fa68e2682032d8a5a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2f155642b4c241b6ba1776ae6de425b6","IPY_MODEL_f88425bfcd204aed952526f56843f06f","IPY_MODEL_4e509895258245dea3de16c4f47e4f3b"],"layout":"IPY_MODEL_189dceb6b3dc47aba13c8614a36f87b2"}},"2f155642b4c241b6ba1776ae6de425b6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_acc7b578812348059780bf199633b8df","placeholder":"","style":"IPY_MODEL_b0a8962084804f40a2d74855cc3bb5e8","value":"Generating test split:  66%"}},"f88425bfcd204aed952526f56843f06f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_19b627703293417d9b5085dd5a508866","max":4358,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5943e00389304257b29c27e0a425c929","value":4358}},"4e509895258245dea3de16c4f47e4f3b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_767d93dbb96845ec9b3c5074f468f261","placeholder":"","style":"IPY_MODEL_edda198d6c2647d5967d6d4e6919eb8b","value":" 2891/4358 [00:00&lt;00:00, 28903.76 examples/s]"}},"189dceb6b3dc47aba13c8614a36f87b2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"acc7b578812348059780bf199633b8df":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b0a8962084804f40a2d74855cc3bb5e8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"19b627703293417d9b5085dd5a508866":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5943e00389304257b29c27e0a425c929":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"767d93dbb96845ec9b3c5074f468f261":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"edda198d6c2647d5967d6d4e6919eb8b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0378235de78a495c8781f6cb299d2094":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d6308eeb79c74ce58686e305a3109256","IPY_MODEL_036e2236edb44a5bbb48bca0d517e044","IPY_MODEL_5b7a2817c9f64c7b8765ae67a2e1933d"],"layout":"IPY_MODEL_0801736cbcb24ca6a1e92822b908fcd6"}},"d6308eeb79c74ce58686e305a3109256":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d970834854be41998125718cf1567d5c","placeholder":"","style":"IPY_MODEL_9b44851383ca434eba7f41d232e82e31","value":"Generating train split: 100%"}},"036e2236edb44a5bbb48bca0d517e044":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_248855b3a77c40a58e4a924aaee8d22a","max":1801350,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e0c3b516c933473cb1e0b23f2643c7a3","value":1801350}},"5b7a2817c9f64c7b8765ae67a2e1933d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d0d88d4a1e5c45c1a4cb7c767d5b761e","placeholder":"","style":"IPY_MODEL_1ca1976a99f4464f8e17e32cb7a08093","value":" 1800569/1801350 [00:44&lt;00:00, 40993.94 examples/s]"}},"0801736cbcb24ca6a1e92822b908fcd6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d970834854be41998125718cf1567d5c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9b44851383ca434eba7f41d232e82e31":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"248855b3a77c40a58e4a924aaee8d22a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e0c3b516c933473cb1e0b23f2643c7a3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d0d88d4a1e5c45c1a4cb7c767d5b761e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1ca1976a99f4464f8e17e32cb7a08093":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d45dca9d9f174044b9a3a36d883e0f61":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_abda310c836143bd896a43e5a7dc16c1","IPY_MODEL_ae56a0f62397484a9f0e678c69fa674b","IPY_MODEL_25aad33a1e2f48ff99aa3fcc0ca50a8c"],"layout":"IPY_MODEL_cd48f56ab40c473e88aae035d0a56d87"}},"abda310c836143bd896a43e5a7dc16c1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e4a9a004afc54252b56e7719ea60f9b4","placeholder":"","style":"IPY_MODEL_d6bcd8eb5bc24396a44f6699e576c7ba","value":"Generating validation split:  82%"}},"ae56a0f62397484a9f0e678c69fa674b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_6aabdbcc9d7b4b759b28e1fce324ea7b","max":3760,"min":0,"orientation":"horizontal","style":"IPY_MODEL_628a82cbe3c6479392ae3caff6867467","value":3760}},"25aad33a1e2f48ff99aa3fcc0ca50a8c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a406c2db19e24dd2960e806e4eaaa510","placeholder":"","style":"IPY_MODEL_b37b47c40096494b992e7bafd3692d03","value":" 3075/3760 [00:00&lt;00:00, 30746.07 examples/s]"}},"cd48f56ab40c473e88aae035d0a56d87":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e4a9a004afc54252b56e7719ea60f9b4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d6bcd8eb5bc24396a44f6699e576c7ba":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6aabdbcc9d7b4b759b28e1fce324ea7b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"628a82cbe3c6479392ae3caff6867467":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a406c2db19e24dd2960e806e4eaaa510":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b37b47c40096494b992e7bafd3692d03":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3312bfd1a1354cb4ad1169f952863a9e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_738cd92cb0874a9eb2627d767274762f","IPY_MODEL_81f0915d99594d609402266f3d8b7940","IPY_MODEL_619f1102c3884f93bbd65efb3cfe70d9"],"layout":"IPY_MODEL_fabb55919e3d4d788e048fe7cc08ef38"}},"738cd92cb0874a9eb2627d767274762f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1121e5de3d0942b3aaf821c2c8631de2","placeholder":"","style":"IPY_MODEL_26266ccec866411a90b585d53baa015f","value":"Downloading: 100%"}},"81f0915d99594d609402266f3d8b7940":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2bd8a237668440c48edb17354f64385c","max":252,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8344ebc1e9e24d5d99185dc9ee744e18","value":252}},"619f1102c3884f93bbd65efb3cfe70d9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f54e3cb459df491fa4fad4ba4aa494b8","placeholder":"","style":"IPY_MODEL_a45dad838bc34b03be1f47c5cf9cbea2","value":" 252/252 [00:00&lt;00:00, 9.04kB/s]"}},"fabb55919e3d4d788e048fe7cc08ef38":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1121e5de3d0942b3aaf821c2c8631de2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"26266ccec866411a90b585d53baa015f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2bd8a237668440c48edb17354f64385c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8344ebc1e9e24d5d99185dc9ee744e18":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f54e3cb459df491fa4fad4ba4aa494b8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a45dad838bc34b03be1f47c5cf9cbea2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9e2c6925d8084d69ab6464512f3617ec":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_52244325f04845fda0497de676feab1b","IPY_MODEL_aba22c48df5c485383677b9e34a8ef35","IPY_MODEL_f70835726cf744adbbed05f682ffcf0c"],"layout":"IPY_MODEL_560accc171c54ab1bb9b2408bc17bc47"}},"52244325f04845fda0497de676feab1b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e268d1d5bc3d4fcb9f7ff2aae8a8e4ee","placeholder":"","style":"IPY_MODEL_00507c26807c4a1f88b55de07a4795ae","value":"Downloading: 100%"}},"aba22c48df5c485383677b9e34a8ef35":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_18325020c20a4d358cc60f5bb26965da","max":689,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4a520d9088894fdda9f254b71e933021","value":689}},"f70835726cf744adbbed05f682ffcf0c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_716671997a6b43c7a1b7bd595aebca33","placeholder":"","style":"IPY_MODEL_38ee3c7e13cb40cd982d352d61bc777f","value":" 689/689 [00:00&lt;00:00, 28.1kB/s]"}},"560accc171c54ab1bb9b2408bc17bc47":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e268d1d5bc3d4fcb9f7ff2aae8a8e4ee":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"00507c26807c4a1f88b55de07a4795ae":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"18325020c20a4d358cc60f5bb26965da":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4a520d9088894fdda9f254b71e933021":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"716671997a6b43c7a1b7bd595aebca33":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"38ee3c7e13cb40cd982d352d61bc777f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3912b4fce23a412f855fac4ca43d5750":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f710e031aebc4973a603ef68c8bdb47d","IPY_MODEL_abc37f6c8f1b4803b076beb1d62308a3","IPY_MODEL_93478367bb634adb82cd7bafb37a2dbb"],"layout":"IPY_MODEL_4d198cd5c2064557bc284bcb96da460a"}},"f710e031aebc4973a603ef68c8bdb47d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_167dad9f5ab34bd39e170cc389bcff6b","placeholder":"","style":"IPY_MODEL_5493b4cd010744d49f5dea5d87033367","value":"Downloading: 100%"}},"abc37f6c8f1b4803b076beb1d62308a3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1bb567fd545a4e0b8ea4978893639624","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ffdd33a86db3479e803d99cb157b9637","value":231508}},"93478367bb634adb82cd7bafb37a2dbb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a953c98f007e417a90109b26d50863ac","placeholder":"","style":"IPY_MODEL_f493ae3308a5476d97fbadf0949311f2","value":" 226k/226k [00:00&lt;00:00, 186kB/s]"}},"4d198cd5c2064557bc284bcb96da460a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"167dad9f5ab34bd39e170cc389bcff6b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5493b4cd010744d49f5dea5d87033367":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1bb567fd545a4e0b8ea4978893639624":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ffdd33a86db3479e803d99cb157b9637":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a953c98f007e417a90109b26d50863ac":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f493ae3308a5476d97fbadf0949311f2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3de100c6cfce435899872c3cad59c5cf":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ddfc7d600ae64543857f85b936608828","IPY_MODEL_8817f4ca8a4b4109b487e3ebf7fe0e0d","IPY_MODEL_e97d653bdcb047d9aa7dc129a1b244d8"],"layout":"IPY_MODEL_855b097c9565486b88a66a3aea0d8206"}},"ddfc7d600ae64543857f85b936608828":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4a4ab8e7953243a6830c483fde47f6ec","placeholder":"","style":"IPY_MODEL_c08b4fd096b341409555ca8e8d5470e3","value":"Downloading: 100%"}},"8817f4ca8a4b4109b487e3ebf7fe0e0d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f2c2596c03a64341b2a4ef98e6cd3cf8","max":112,"min":0,"orientation":"horizontal","style":"IPY_MODEL_928d565279d0416e85c9a8ce3a88c3d4","value":112}},"e97d653bdcb047d9aa7dc129a1b244d8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_30c3721218aa4c368d51e7d6f53a3153","placeholder":"","style":"IPY_MODEL_abf3ed128a77443ba38b51b74249661a","value":" 112/112 [00:00&lt;00:00, 4.09kB/s]"}},"855b097c9565486b88a66a3aea0d8206":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4a4ab8e7953243a6830c483fde47f6ec":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c08b4fd096b341409555ca8e8d5470e3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f2c2596c03a64341b2a4ef98e6cd3cf8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"928d565279d0416e85c9a8ce3a88c3d4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"30c3721218aa4c368d51e7d6f53a3153":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"abf3ed128a77443ba38b51b74249661a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":5}